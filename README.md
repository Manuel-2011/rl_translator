# Improving Low-Resource Translation with Dictionary-Guided Fine-Tuning and RL: A Spanish-to-Wayuunaiki Study
We propose a novel approach to machine translation for low-resource languages by integrating large language models (LLMs) with external linguistic tools. Focusing on the Spanish–Wayuunaiki language pair, we frame translation as a tool-augmented decision-making problem, wherein the model can selectively consult a dictionary during the translation process. Our method combines supervised fine-tuning with reinforcement learning based on the Guided Reward Policy Optimization (GRPO) algorithm, enabling an instruction-tuned model to learn both when and how to use the external tool effectively. To align model behavior with translation quality, we leverage GRPO’s reward mechanism, guided by BLEU scores. To assess the impact of model architecture and training strategy, we conduct ablation studies on our training pipeline and compare Qwen2.5-0.5B-Instruct with other models, including LLaMA and a prior system based on the NLLB model. Preliminary results demonstrate that instruction-tuned models with tool access, further refined through reinforcement learning, achieve state-of-the-art performance on the Spanish–Wayuunaiki test set. These findings underscore the potential of LLM-based agents augmented with external tools to improve translation quality in low-resource language settings.

## Repository structure
This repository has the following relevant folders:
- datasets: This is where training and evaluation files are located. In csv format, with a single-column file per language and split (train, dev).
- logs: This folder stores the logs generated by the training process.
- models: This folders stores the trained models, or model checkpoints to start from, as PyTorch model folders.
- Training files: The files starting with grpo_trainer... are the files that fine-tune LLM to translate Spanish to Wayuunaiki. **The usage of these files depends on the model trained (view branches section)**.

## Setup
Setting up the environment is the same process for both branches. Follow these instructions:
1. Create a python environment with `python -m venv venv`.
2. Activate the environment with `source venv/bin/activate`
3. Install the requirements with `pip install -r requirements.txt`

## Branches
The main branch was the initial setup agreed by the team to develop several training configurations. As there are two types of models trained, two branches were generated.

### feature/adapt_training_to_task
This branch contains the fine-tuning of Qwen and Llama based models. To proceed with the fine-tuning 
