# Improving Low-Resource Translation with Dictionary-Guided Fine-Tuning and RL: A Spanish-to-Wayuunaiki Study
We propose a novel approach to machine translation for low-resource languages by integrating large language models (LLMs) with external linguistic tools. Focusing on the Spanish–Wayuunaiki language pair, we frame translation as a tool-augmented decision-making problem, wherein the model can selectively consult a dictionary during the translation process. Our method combines supervised fine-tuning with reinforcement learning based on the Guided Reward Policy Optimization (GRPO) algorithm, enabling an instruction-tuned model to learn both when and how to use the external tool effectively. To align model behavior with translation quality, we leverage GRPO’s reward mechanism, guided by BLEU scores. To assess the impact of model architecture and training strategy, we conduct ablation studies on our training pipeline and compare Qwen2.5-0.5B-Instruct with other models, including LLaMA and a prior system based on the NLLB model. Preliminary results demonstrate that instruction-tuned models with tool access, further refined through reinforcement learning, achieve state-of-the-art performance on the Spanish–Wayuunaiki test set. These findings underscore the potential of LLM-based agents augmented with external tools to improve translation quality in low-resource language settings.

## Repository structure
This repository has the following relevant folders:
- datasets: This is where training and evaluation files are located. In csv format, with a single-column file per language and split (train, dev).
- logs: This folder stores the logs generated by the training process.
- models: This folders stores the trained models, or model checkpoints to start from, as PyTorch model folders.
- Training files: The files starting with grpo_trainer... are the files that fine-tune LLM to translate Spanish to Wayuunaiki. **The usage of these files depends on the fine-tuned model**.

## Hyperparameters
There are two types of hyperparameters, some for model training and some for paths (depending on the file they may or not may appear). Make sure you modify them to the values you need before running the experiments (or changing them if something failed).

### Training hyperparameters

| **Hyperparameter**            | **Definition**                                                                                          | **Value**      |
|-------------------------------|---------------------------------------------------------------------------------------------------------|----------------|
| max_steps                     | Maximum number of examples seen                                                                          | 80000          |
| sims_per_prompt               | Simulations to calculate reward per example                                                              | 8              |
| policy_lr                     | Learning rate for the policy update                                                                      | 5e-6           |
| kl_penalty_coef              | Penalty coefficient to avoid a strong variation of the updated model compared with the reference model   | 0.04           |
| temperature                   | Temperature of the LLM for generations                                                                   | 1.0            |
| lower_clip                    | Minimum value for the loss function in the update policy                                                 | 0.8            |
| upper_clip                    | Maximum value for the loss function in the update policy                                                 | 1.2            |
| max_new_tokens                | Maximum tokens generated by the LLM                                                                      | 512            |
| r                             | Rank of the approximation matrices used for LoRA                                                         | 64             |
| lora_alpha                    | Scaling factor for LoRA approximation matrices                                                           | 64             |
| optimizer                     | Type of optimizer                                                                                        | AdamW          |
| policy_lr                     | Learning rate of the optimizer                                                                           | 5e-6           |
| betas                         | Optimizer beta                                                                                           | (0.9, 0.999)   |
| eps                           | Optimizer epsilon                                                                                       | 1e-8           |
| weight_decay                  | Optimizer weight decay                                                                                    | 0.0            |
| gradient_clipping             | Optimizer gradient clipping                                                                               | 0.1            |

### Path hyperparameters
- save_adapter_path: Path to save model after all steps.
- best_adapter_path: Path to save best step model.
- base_model_name: Path or huggingface model id to load model. 
- spanish_train_file: Spanish training dataset path.
- wayuu_train_file: Wayuunaiki training dataset path.
- spanish_val_file:  Spanish validation dataset path.
- wayuu_val_file: Wayuunaiki validation dataset path.
- dataset_path: Dataset path for Supervised fine-tuning.
- model_name: Model name for Supervised fine-tuning.
- best_checkpoint_name/checkpoint_to_start: Path of a checkpoint of a model to start fine-tuning from. 
## Setup
Setting up the environment is the same process for both branches. Follow these instructions:
1. Create a python environment with `python -m venv venv`.
2. Activate the environment with `source venv/bin/activate`
3. Install the requirements with `pip install -r requirements.txt`

## Fine-tuning models
Depending on the type of model to be fine-tuned, different scripts must be used.

### Decoder only models (Qwen-Llama)
Run the following scripts in order:
1. [create_sft_dataset.py](create_sft_dataset.py): Generates a pkl dataset for Supervised fine-tuning.
2. [sft_trainer.py](sft_trainer.py): Fine-tunes a model in a supervised fashion to teach it how to use the dictionary tool.
3. [evaluation.py](evaluation.py): Evaluates the model on the validation set using the BLEU score metric.
