{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffda8ffc",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e904aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, spa_path, wayuu_path):\n",
    "        with open(spa_path, 'r', encoding='utf-8') as f:\n",
    "            self.spa_lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "        with open(wayuu_path, 'r', encoding='utf-8') as f:\n",
    "            self.wayuu_lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spa_lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        spa = self.spa_lines[idx]\n",
    "        wayuu = self.wayuu_lines[idx]\n",
    "        \n",
    "        return spa, wayuu\n",
    "    \n",
    "spanish_val_file = 'datasets/dev.es.txt'\n",
    "wayuu_val_file = 'datasets/dev.guc.txt'\n",
    "\n",
    "# Load the dataset\n",
    "dataset = TextDataset(spanish_val_file, wayuu_val_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d81292",
   "metadata": {},
   "source": [
    "### Eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bf39269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manuel/projects/rl_translator/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-26 12:34:20,724\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sacrebleu\n",
    "from tqdm import tqdm\n",
    "from vllm import SamplingParams\n",
    "\n",
    "def get_rewards_translation(generations, correct_translations):\n",
    "\n",
    "    bleu = sacrebleu.BLEU(effective_order = True)\n",
    "    def get_bleu_score(sample, correct_translation):\n",
    "        # Compute bleu score for each sample. \n",
    "        # Bleu score normalized to [0, 1]\n",
    "        return bleu.sentence_score(sample, \n",
    "                                   [correct_translation]\n",
    "                                   ).score\n",
    "\n",
    "    answer_bleu_scores = [\n",
    "        get_bleu_score(sample, translation)\n",
    "        for sample, translation in zip(generations, correct_translations)\n",
    "    ]\n",
    "    \n",
    "    return answer_bleu_scores\n",
    "\n",
    "translate_prompt_template_tool=\"\"\"Translate the following Spanish text into Wayuunaiki.\n",
    "Begin by identifying any words or phrases you're unsure how to translate. Then, you may look up those words using the dictionary tool by wrapping the Spanish word in <spa_to_wayuu> and </spa_to_wayuu>,\n",
    "and doind that for every unknown word. The dictionary will return matches enclosed in <matches> and </matches>. You can use the dictionary as many times as necessary.\n",
    "Once you have all the information you need, provide the final translation enclosed in <answer> and </answer>. For example: <answer> xxx </answer>.\n",
    "\n",
    "Spanish text: {}\"\"\"\n",
    "def generate_batch_completion(model, tokenizer, prompts: list, actions_num=1, custom_prompt_template=None, **kwargs):\n",
    "    prompt_template = custom_prompt_template if custom_prompt_template else translate_prompt_template_tool\n",
    "    batch = [[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_template.format(prompt)}\n",
    "    ] for prompt in prompts]\n",
    "    texts = tokenizer.apply_chat_template(\n",
    "        batch,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    default_sampling_args = {\n",
    "        'max_new_tokens': 512,\n",
    "        'temperature': 0.8,\n",
    "        'top_p': 0.95,\n",
    "    }\n",
    "    default_sampling_args.update(kwargs)\n",
    "\n",
    "    model_inputs = tokenizer(texts)\n",
    "\n",
    "    inputs = model_inputs.input_ids\n",
    "    dones = [False] * len(prompts)\n",
    "    prompt_length = [len(input_ids) for input_ids in inputs]\n",
    "    mask = [[1] * len(input_ids) for input_ids in inputs]\n",
    "    responses = [\"\"] * len(prompts)\n",
    "    tools_enabled = kwargs.get('tools', [])\n",
    "    tools_enabled = [] if tools_enabled is None else tools_enabled\n",
    "    stop_tokens = [tool['end_token'] for tool in tools_enabled]\n",
    "    tool_used = [False] * len(prompts)\n",
    "    how_many_tool_calls = [0] * len(prompts)\n",
    "    for action_step in range(actions_num + 1 if len(tools_enabled) > 0 else 1):\n",
    "        sampling_params = SamplingParams(temperature=default_sampling_args[\"temperature\"], top_p=default_sampling_args['top_p'], top_k=-1, max_tokens=default_sampling_args['max_new_tokens'],\n",
    "            stop=stop_tokens)\n",
    "        outputs = model.generate(prompt_token_ids=inputs, sampling_params=sampling_params, lora_request=kwargs['lora_request'], use_tqdm=False)\n",
    "\n",
    "        for j, output in enumerate(outputs):\n",
    "            if dones[j]:\n",
    "                continue\n",
    "            \n",
    "            for tool in tools_enabled:\n",
    "                if output.outputs[0].stop_reason == tool['end_token'] and tool['start_token'] in output.outputs[0].text:\n",
    "                    api_args = output.outputs[0].text.split(tool['start_token'])[1].strip()\n",
    "                    api_result = tool['api'](api_args)\n",
    "                    # responses[j] += f\"{tool['start_token']} \" + api_args + f\" {tool['end_token']}\" + api_result\n",
    "                    responses[j] += output.outputs[0].text + f\"{tool['end_token']}\" + api_result\n",
    "                    api_result_tokens = tokenizer.encode(api_result, return_tensors=None)\n",
    "                    inputs[j] += list(output.outputs[0].token_ids) + api_result_tokens\n",
    "\n",
    "                    tool_used[j] = True\n",
    "                    how_many_tool_calls[j] += 1\n",
    "                    break # Only one tool can be used at a time\n",
    "            if output.outputs[0].finish_reason == \"stop\" and output.outputs[0].stop_reason is None:\n",
    "                responses[j] += output.outputs[0].text\n",
    "                inputs[j] += list(output.outputs[0].token_ids)\n",
    "                dones[j] = True\n",
    "            elif output.outputs[0].stop_reason not in stop_tokens:\n",
    "                print(f\"Unexpected finish reason: {output.outputs[0].finish_reason} {output.outputs[0].stop_reason}\")\n",
    "                responses[j] += tokenizer.eos_token\n",
    "                inputs[j] += [tokenizer.eos_token_id]\n",
    "                dones[j] = True\n",
    "        \n",
    "        if all(dones):\n",
    "            break\n",
    "\n",
    "    return responses, tool_used, how_many_tool_calls\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_answer(response, transform_fn = lambda x: x, nan_val = None)->str|None:\n",
    "    ans = re.match('.*?<answer>(.*?)</answer>\\.?\\s*$', response, re.DOTALL|re.MULTILINE)\n",
    "    if ans:\n",
    "        try:\n",
    "            return transform_fn(ans[1].strip())\n",
    "        except:\n",
    "            return nan_val\n",
    "    return nan_val\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataloader, actions_num=1, lora_request=None, tools=None, custom_prompt_template=None):\n",
    "    sum_bleu = 0\n",
    "    num_samples = 0\n",
    "    tools_used_in_total = 0\n",
    "    calls_per_sample = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            inputs, targets = batch\n",
    "\n",
    "            # Generate translations\n",
    "            outputs, tools_used, how_many_tool_calls = generate_batch_completion(model, tokenizer, inputs, actions_num=actions_num, lora_request=lora_request, tools=tools, temperature=0, top_p=1, max_new_tokens=768, custom_prompt_template=custom_prompt_template)\n",
    "\n",
    "            tools_used_in_total += sum(tools_used)\n",
    "            calls_per_sample += sum(how_many_tool_calls)\n",
    "\n",
    "            generated_translations = [\n",
    "                extract_answer(output, transform_fn=lambda x: x.strip(), nan_val='')\n",
    "                for output in outputs\n",
    "            ]\n",
    "            # Calculate BLEU scores\n",
    "            bleu_scores = get_rewards_translation(generated_translations, targets)\n",
    "            \n",
    "            sum_bleu += sum(bleu_scores)\n",
    "            num_samples += len(bleu_scores)\n",
    "    avg_bleu = sum_bleu / num_samples if num_samples > 0 else 0\n",
    "    tools_used_avg = tools_used_in_total / num_samples\n",
    "    calls_per_sample_avg = calls_per_sample / tools_used_in_total if tools_used_in_total > 0 else 0\n",
    "    return avg_bleu, tools_used_avg, calls_per_sample_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0681dd90",
   "metadata": {},
   "source": [
    "### tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a3e47b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spa_to_wayu_dictionary(spanish_word, max_matches=5):\n",
    "    dictionary_path = 'assets/spanish_to_wayuunaiki_short.csv'\n",
    "\n",
    "    with open(dictionary_path, 'r', encoding='utf-8') as f:\n",
    "        all_matches = []\n",
    "        line = f.readline()\n",
    "        while line != '' and len(all_matches) < max_matches:\n",
    "            data = line.strip().split(',')\n",
    "            if re.search(rf'\\b{re.escape(spanish_word)}\\b', data[0], re.IGNORECASE):\n",
    "                all_matches.append(data)\n",
    "            line = f.readline()\n",
    "\n",
    "    if len(all_matches) > 0:\n",
    "        result = \" <matches> \" + '\\n'.join(f'{spa}: {wayuu}' for spa, wayuu in all_matches) + \" </matches>\"\n",
    "        # print(f'CORRECT USE OF SPA_TO_WAYU TOOL. Word: {spanish_word}, Result: {result}')\n",
    "    else:\n",
    "        result = \" <matches> No matches found </matches>\"\n",
    "        # print(f'NO_MATCHES SPA_TO_WAYU TOOL. Word: {spanish_word}')\n",
    "\n",
    "    return result\n",
    "\n",
    "TOOLS = [\n",
    "    {\n",
    "        'name': 'spa_to_wayu',\n",
    "        'description': 'A tool that translates a word from Spanish to Wayuunaiki.',\n",
    "        'api': spa_to_wayu_dictionary,\n",
    "        'start_token': '<spa_to_wayuu>',\n",
    "        'end_token': '</spa_to_wayuu>',\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191b95bc",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66d76726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-26 12:34:23,894] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 12:34:25 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 05-26 12:34:29 config.py:549] This model supports multiple tasks: {'generate', 'reward', 'classify', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 05-26 12:34:29 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='Qwen/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2060, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-0.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 05-26 12:34:30 interface.py:304] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 05-26 12:34:30 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 05-26 12:34:30 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-0.5B-Instruct...\n",
      "INFO 05-26 12:34:30 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 05-26 12:34:30 weight_utils.py:304] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.94it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 12:34:31 model_runner.py:1115] Loading model weights took 0.9277 GB\n",
      "INFO 05-26 12:34:31 punica_selector.py:18] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 12:34:32 worker.py:267] Memory profiling takes 0.90 seconds\n",
      "INFO 05-26 12:34:32 worker.py:267] the current vLLM instance can use total_gpu_memory (11.99GiB) x gpu_memory_utilization (0.20) = 2.40GiB\n",
      "INFO 05-26 12:34:32 worker.py:267] model weights take 0.93GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 0.04GiB.\n",
      "INFO 05-26 12:34:32 executor_base.py:111] # cuda blocks: 233, # CPU blocks: 32768\n",
      "INFO 05-26 12:34:32 executor_base.py:116] Maximum concurrency for 2060 tokens per request: 1.81x\n",
      "INFO 05-26 12:34:32 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:11<00:00,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 12:34:44 model_runner.py:1562] Graph capturing finished in 12 secs, took 0.31 GiB\n",
      "INFO 05-26 12:34:44 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 13.05 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/104 [00:00<?, ?it/s]/tmp/ipykernel_913215/1015541555.py:118: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.\n",
      "  outputs, tools_used, how_many_tool_calls = generate_batch_completion(model, tokenizer, inputs, actions_num=actions_num, lora_request=lora_request, tools=tools, temperature=0, top_p=1, max_new_tokens=768, custom_prompt_template=custom_prompt_template)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:34:45 scheduler.py:1754] Sequence group 45 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n",
      "WARNING 05-26 12:34:48 scheduler.py:1754] Sequence group 23 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/104 [00:12<21:10, 12.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:34:57 scheduler.py:1754] Sequence group 111 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n",
      "WARNING 05-26 12:35:05 scheduler.py:1754] Sequence group 124 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/104 [00:25<21:28, 12.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:35:15 scheduler.py:1754] Sequence group 152 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/104 [00:37<21:17, 12.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:35:23 scheduler.py:1754] Sequence group 234 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=251\n",
      "WARNING 05-26 12:35:31 scheduler.py:1754] Sequence group 245 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/104 [00:53<23:20, 14.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:35:39 scheduler.py:1754] Sequence group 308 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=351\n",
      "WARNING 05-26 12:35:47 scheduler.py:1754] Sequence group 307 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 5/104 [01:06<22:15, 13.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:35:53 scheduler.py:1754] Sequence group 341 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/104 [01:18<21:26, 13.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:36:04 scheduler.py:1754] Sequence group 433 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=501\n",
      "WARNING 05-26 12:36:11 scheduler.py:1754] Sequence group 431 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/104 [01:33<22:12, 13.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:36:19 scheduler.py:1754] Sequence group 510 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=601\n",
      "WARNING 05-26 12:36:27 scheduler.py:1754] Sequence group 509 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/104 [01:46<21:10, 13.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:36:36 scheduler.py:1754] Sequence group 545 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 9/104 [02:00<21:24, 13.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:36:45 scheduler.py:1754] Sequence group 625 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=751\n",
      "WARNING 05-26 12:36:51 scheduler.py:1754] Sequence group 623 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 10/104 [02:14<21:40, 13.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:37:00 scheduler.py:1754] Sequence group 698 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=851\n",
      "WARNING 05-26 12:37:08 scheduler.py:1754] Sequence group 701 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/104 [02:29<21:40, 13.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:37:15 scheduler.py:1754] Sequence group 742 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/104 [02:40<20:22, 13.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:37:26 scheduler.py:1754] Sequence group 812 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1001\n",
      "WARNING 05-26 12:37:32 scheduler.py:1754] Sequence group 817 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 13/104 [02:52<19:38, 12.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:37:39 scheduler.py:1754] Sequence group 864 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 14/104 [03:06<19:43, 13.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:37:51 scheduler.py:1754] Sequence group 938 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1151\n",
      "WARNING 05-26 12:37:58 scheduler.py:1754] Sequence group 938 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 15/104 [03:20<19:54, 13.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:38:06 scheduler.py:1754] Sequence group 1010 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 16/104 [03:32<19:11, 13.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:38:17 scheduler.py:1754] Sequence group 1063 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1301\n",
      "WARNING 05-26 12:38:24 scheduler.py:1754] Sequence group 1058 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 17/104 [03:47<19:49, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:38:33 scheduler.py:1754] Sequence group 1132 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1401\n",
      "WARNING 05-26 12:38:39 scheduler.py:1754] Sequence group 1142 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 18/104 [03:59<18:34, 12.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:38:45 scheduler.py:1754] Sequence group 1212 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 19/104 [04:11<17:55, 12.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:38:56 scheduler.py:1754] Sequence group 1256 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1551\n",
      "WARNING 05-26 12:39:02 scheduler.py:1754] Sequence group 1272 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 20/104 [04:25<18:29, 13.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:39:11 scheduler.py:1754] Sequence group 1329 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1651\n",
      "WARNING 05-26 12:39:19 scheduler.py:1754] Sequence group 1336 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 21/104 [04:40<18:54, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:39:26 scheduler.py:1754] Sequence group 1366 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1751\n",
      "WARNING 05-26 12:39:35 scheduler.py:1754] Sequence group 1399 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 22/104 [04:56<19:45, 14.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:39:42 scheduler.py:1754] Sequence group 1448 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1851\n",
      "WARNING 05-26 12:39:53 scheduler.py:1754] Sequence group 1470 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 23/104 [05:10<19:18, 14.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:40:02 scheduler.py:1754] Sequence group 1531 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 24/104 [05:21<17:48, 13.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:40:09 scheduler.py:1754] Sequence group 1550 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 25/104 [05:33<17:00, 12.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:40:19 scheduler.py:1754] Sequence group 1646 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2051\n",
      "WARNING 05-26 12:40:27 scheduler.py:1754] Sequence group 1645 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 26/104 [05:48<17:25, 13.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:40:35 scheduler.py:1754] Sequence group 1705 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 27/104 [05:58<15:59, 12.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:40:44 scheduler.py:1754] Sequence group 1779 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2201\n",
      "WARNING 05-26 12:40:52 scheduler.py:1754] Sequence group 1791 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 28/104 [06:12<16:26, 12.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:40:59 scheduler.py:1754] Sequence group 1829 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 29/104 [06:23<15:12, 12.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:41:09 scheduler.py:1754] Sequence group 1915 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 30/104 [06:32<13:57, 11.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:41:17 scheduler.py:1754] Sequence group 1978 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 31/104 [06:43<13:35, 11.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:41:28 scheduler.py:1754] Sequence group 2023 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2451\n",
      "WARNING 05-26 12:41:34 scheduler.py:1754] Sequence group 2032 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 32/104 [06:54<13:29, 11.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:41:40 scheduler.py:1754] Sequence group 2088 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 33/104 [07:06<13:30, 11.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:41:51 scheduler.py:1754] Sequence group 2163 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 34/104 [07:17<13:13, 11.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:42:02 scheduler.py:1754] Sequence group 2216 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2651\n",
      "WARNING 05-26 12:42:08 scheduler.py:1754] Sequence group 2207 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 35/104 [07:33<14:47, 12.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:42:18 scheduler.py:1754] Sequence group 2281 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2751\n",
      "WARNING 05-26 12:42:24 scheduler.py:1754] Sequence group 2279 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 36/104 [07:46<14:37, 12.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:42:32 scheduler.py:1754] Sequence group 2355 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2851\n",
      "WARNING 05-26 12:42:40 scheduler.py:1754] Sequence group 2348 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 37/104 [08:00<14:46, 13.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:42:47 scheduler.py:1754] Sequence group 2401 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2951\n",
      "WARNING 05-26 12:42:56 scheduler.py:1754] Sequence group 2422 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 38/104 [08:15<15:02, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 39/104 [08:23<13:04, 12.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:43:09 scheduler.py:1754] Sequence group 2536 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3051\n",
      "WARNING 05-26 12:43:15 scheduler.py:1754] Sequence group 2541 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 40/104 [08:39<13:53, 13.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:43:24 scheduler.py:1754] Sequence group 2602 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3151\n",
      "WARNING 05-26 12:43:30 scheduler.py:1754] Sequence group 2618 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 41/104 [08:51<13:19, 12.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:43:37 scheduler.py:1754] Sequence group 2643 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 42/104 [09:04<13:28, 13.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:43:49 scheduler.py:1754] Sequence group 2730 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3301\n",
      "WARNING 05-26 12:43:55 scheduler.py:1754] Sequence group 2723 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 43/104 [09:17<13:13, 13.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:44:03 scheduler.py:1754] Sequence group 2815 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 44/104 [09:30<12:44, 12.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:44:14 scheduler.py:1754] Sequence group 2856 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3451\n",
      "WARNING 05-26 12:44:21 scheduler.py:1754] Sequence group 2851 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3501\n",
      "WARNING 05-26 12:44:28 scheduler.py:1754] Sequence group 2873 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 45/104 [09:46<13:35, 13.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:44:36 scheduler.py:1754] Sequence group 2910 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 46/104 [10:00<13:20, 13.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:44:45 scheduler.py:1754] Sequence group 2997 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3651\n",
      "WARNING 05-26 12:44:53 scheduler.py:1754] Sequence group 3002 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 47/104 [10:11<12:19, 12.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:45:00 scheduler.py:1754] Sequence group 3054 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 48/104 [10:22<11:30, 12.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:45:08 scheduler.py:1754] Sequence group 3101 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3801\n",
      "WARNING 05-26 12:45:16 scheduler.py:1754] Sequence group 3126 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 49/104 [10:37<12:10, 13.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:45:23 scheduler.py:1754] Sequence group 3163 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3901\n",
      "WARNING 05-26 12:45:31 scheduler.py:1754] Sequence group 3187 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 50/104 [10:54<12:50, 14.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:45:39 scheduler.py:1754] Sequence group 3256 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=4001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 51/104 [11:05<11:43, 13.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:45:50 scheduler.py:1754] Sequence group 3307 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=4051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 52/104 [11:14<10:24, 12.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:45:59 scheduler.py:1754] Sequence group 3369 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=4101\n",
      "WARNING 05-26 12:46:05 scheduler.py:1754] Sequence group 3369 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=4151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 53/104 [11:29<10:58, 12.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:46:14 scheduler.py:1754] Sequence group 3436 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=4201\n",
      "WARNING 05-26 12:46:21 scheduler.py:1754] Sequence group 3436 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=4251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 54/104 [11:42<10:46, 12.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:46:29 scheduler.py:1754] Sequence group 3472 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=4301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 55/104 [11:56<10:56, 13.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:46:41 scheduler.py:1754] Sequence group 3563 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=4351\n",
      "WARNING 05-26 12:46:47 scheduler.py:1754] Sequence group 3572 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=4401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 56/104 [12:08<10:22, 12.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:46:54 scheduler.py:1754] Sequence group 3635 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=4451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 57/104 [12:22<10:28, 13.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:47:07 scheduler.py:1754] Sequence group 3691 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=4501\n",
      "WARNING 05-26 12:47:14 scheduler.py:1754] Sequence group 3706 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=4551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 58/104 [12:32<09:29, 12.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:47:19 scheduler.py:1754] Sequence group 3741 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=4601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 59/104 [12:44<09:12, 12.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:47:30 scheduler.py:1754] Sequence group 3827 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=4651\n",
      "WARNING 05-26 12:47:37 scheduler.py:1754] Sequence group 3826 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=4701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 60/104 [12:58<09:20, 12.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:47:47 scheduler.py:1754] Sequence group 3882 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=4751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 61/104 [13:08<08:34, 11.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:47:55 scheduler.py:1754] Sequence group 3949 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=4801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 62/104 [13:21<08:27, 12.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:48:06 scheduler.py:1754] Sequence group 4014 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=4851\n",
      "WARNING 05-26 12:48:13 scheduler.py:1754] Sequence group 4029 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=4901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 63/104 [13:32<08:05, 11.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:48:24 scheduler.py:1754] Sequence group 4064 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=4951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 64/104 [13:47<08:35, 12.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:48:33 scheduler.py:1754] Sequence group 4140 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=5001\n",
      "WARNING 05-26 12:48:39 scheduler.py:1754] Sequence group 4144 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=5051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 65/104 [13:59<08:10, 12.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:48:47 scheduler.py:1754] Sequence group 4174 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=5101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 66/104 [14:11<07:51, 12.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:48:57 scheduler.py:1754] Sequence group 4281 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=5151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 67/104 [14:23<07:26, 12.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:49:07 scheduler.py:1754] Sequence group 4330 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=5201\n",
      "WARNING 05-26 12:49:13 scheduler.py:1754] Sequence group 4327 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=5251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 68/104 [14:35<07:18, 12.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:49:20 scheduler.py:1754] Sequence group 4404 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=5301\n",
      "WARNING 05-26 12:49:28 scheduler.py:1754] Sequence group 4404 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=5351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 69/104 [14:48<07:11, 12.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:49:37 scheduler.py:1754] Sequence group 4457 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=5401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 70/104 [14:59<06:44, 11.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:49:45 scheduler.py:1754] Sequence group 4536 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=5451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 71/104 [15:10<06:29, 11.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:49:55 scheduler.py:1754] Sequence group 4594 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=5501\n",
      "WARNING 05-26 12:50:03 scheduler.py:1754] Sequence group 4583 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=5551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 72/104 [15:24<06:41, 12.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:50:10 scheduler.py:1754] Sequence group 4667 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=5601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 73/104 [15:35<06:09, 11.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:50:20 scheduler.py:1754] Sequence group 4718 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=5651\n",
      "WARNING 05-26 12:50:26 scheduler.py:1754] Sequence group 4728 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=5701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 74/104 [15:47<05:59, 12.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:50:33 scheduler.py:1754] Sequence group 4793 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=5751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 75/104 [16:00<05:56, 12.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:50:45 scheduler.py:1754] Sequence group 4841 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=5801\n",
      "WARNING 05-26 12:50:52 scheduler.py:1754] Sequence group 4861 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=5851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 76/104 [16:10<05:27, 11.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:51:00 scheduler.py:1754] Sequence group 4897 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=5901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 77/104 [16:22<05:13, 11.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:51:07 scheduler.py:1754] Sequence group 4991 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=5951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 78/104 [16:31<04:39, 10.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:51:16 scheduler.py:1754] Sequence group 5045 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=6001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 79/104 [16:42<04:36, 11.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:51:27 scheduler.py:1754] Sequence group 5099 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=6051\n",
      "WARNING 05-26 12:51:33 scheduler.py:1754] Sequence group 5117 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=6101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 80/104 [16:54<04:27, 11.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:51:39 scheduler.py:1754] Sequence group 5169 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=6151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 81/104 [17:06<04:21, 11.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:51:51 scheduler.py:1754] Sequence group 5229 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=6201\n",
      "WARNING 05-26 12:51:56 scheduler.py:1754] Sequence group 5221 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=6251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 82/104 [17:17<04:12, 11.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:52:03 scheduler.py:1754] Sequence group 5297 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=6301\n",
      "WARNING 05-26 12:52:11 scheduler.py:1754] Sequence group 5309 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=6351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 83/104 [17:32<04:21, 12.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:52:21 scheduler.py:1754] Sequence group 5330 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=6401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 84/104 [17:45<04:14, 12.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:52:31 scheduler.py:1754] Sequence group 5426 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=6451\n",
      "WARNING 05-26 12:52:38 scheduler.py:1754] Sequence group 5416 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=6501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 85/104 [17:59<04:05, 12.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:52:46 scheduler.py:1754] Sequence group 5462 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=6551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 86/104 [18:08<03:31, 11.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:52:54 scheduler.py:1754] Sequence group 5549 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=6601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 87/104 [18:20<03:21, 11.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:53:05 scheduler.py:1754] Sequence group 5610 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=6651\n",
      "WARNING 05-26 12:53:10 scheduler.py:1754] Sequence group 5613 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=6701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 88/104 [18:32<03:10, 11.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:53:17 scheduler.py:1754] Sequence group 5679 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=6751\n",
      "WARNING 05-26 12:53:25 scheduler.py:1754] Sequence group 5689 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=6801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 89/104 [18:47<03:11, 12.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:53:33 scheduler.py:1754] Sequence group 5746 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=6851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 90/104 [18:58<02:54, 12.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:53:43 scheduler.py:1754] Sequence group 5797 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=6901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 91/104 [19:07<02:26, 11.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:53:52 scheduler.py:1754] Sequence group 5871 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=6951\n",
      "WARNING 05-26 12:53:58 scheduler.py:1754] Sequence group 5880 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=7001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 92/104 [19:19<02:18, 11.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:54:06 scheduler.py:1754] Sequence group 5903 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=7051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 93/104 [19:34<02:16, 12.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:54:18 scheduler.py:1754] Sequence group 5993 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=7101\n",
      "WARNING 05-26 12:54:25 scheduler.py:1754] Sequence group 5997 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=7151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 94/104 [19:48<02:10, 13.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:54:33 scheduler.py:1754] Sequence group 6058 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=7201\n",
      "WARNING 05-26 12:54:42 scheduler.py:1754] Sequence group 6063 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=7251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 95/104 [20:03<02:02, 13.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:54:49 scheduler.py:1754] Sequence group 6137 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=7301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 96/104 [20:15<01:44, 13.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:55:00 scheduler.py:1754] Sequence group 6186 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=7351\n",
      "WARNING 05-26 12:55:05 scheduler.py:1754] Sequence group 6176 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=7401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 97/104 [20:27<01:29, 12.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:55:13 scheduler.py:1754] Sequence group 6261 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=7451\n",
      "WARNING 05-26 12:55:21 scheduler.py:1754] Sequence group 6269 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=7501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 98/104 [20:39<01:15, 12.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:55:31 scheduler.py:1754] Sequence group 6330 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=7551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 99/104 [20:49<00:59, 11.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:55:36 scheduler.py:1754] Sequence group 6364 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=7601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 100/104 [21:01<00:47, 11.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:55:47 scheduler.py:1754] Sequence group 6448 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=7651\n",
      "WARNING 05-26 12:55:54 scheduler.py:1754] Sequence group 6446 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=7701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 101/104 [21:15<00:37, 12.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:56:01 scheduler.py:1754] Sequence group 6525 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=7751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 102/104 [21:27<00:24, 12.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:56:12 scheduler.py:1754] Sequence group 6569 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=7801\n",
      "WARNING 05-26 12:56:18 scheduler.py:1754] Sequence group 6579 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=7851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 103/104 [21:40<00:12, 12.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [21:48<00:00, 12.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Average BLEU score: 0.1028\n",
      "Average tools used: 0.0000\n",
      "Average calls per sample: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from pretrained peft model\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "translate_prompt_template=\"\"\"Translate the following Spanish text into Wayuunaiki. Provide the final translation enclosed in <answer> and </answer>. For example: <answer> xxx </answer>.\n",
    "Spanish text: {}\"\"\"\n",
    "\n",
    "base_model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "inference_engine = LLM(\n",
    "    model=base_model_name,\n",
    "    enable_lora=True,\n",
    "    max_lora_rank=64,\n",
    "    max_loras=1,\n",
    "    gpu_memory_utilization=0.2,\n",
    "    # enable_prefix_caching=True,\n",
    "    swap_space=6,\n",
    "    scheduling_policy=\"fcfs\",\n",
    "    dtype=torch.bfloat16,\n",
    "    max_model_len=2060,\n",
    "    # enable_sleep_mode=True,\n",
    "    )\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "avg_bleu, tools_used_avg, calls_per_sample_avg = evaluate_model(inference_engine, tokenizer, dataloader, custom_prompt_template=translate_prompt_template)\n",
    "print(f\"Average BLEU score: {avg_bleu:.4f}\")\n",
    "print(f\"Average tools used: {tools_used_avg:.4f}\")\n",
    "print(f\"Average calls per sample: {calls_per_sample_avg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34aa9f0",
   "metadata": {},
   "source": [
    "## Base Model + tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbc5fbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-26 12:08:59,709] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 12:09:01 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 05-26 12:09:06 config.py:549] This model supports multiple tasks: {'score', 'classify', 'generate', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 05-26 12:09:06 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='Qwen/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2060, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-0.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 05-26 12:09:06 interface.py:304] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 05-26 12:09:06 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 05-26 12:09:07 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-0.5B-Instruct...\n",
      "INFO 05-26 12:09:07 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 05-26 12:09:08 weight_utils.py:304] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.32it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 12:09:08 model_runner.py:1115] Loading model weights took 0.9277 GB\n",
      "INFO 05-26 12:09:08 punica_selector.py:18] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 12:09:10 worker.py:267] Memory profiling takes 1.00 seconds\n",
      "INFO 05-26 12:09:10 worker.py:267] the current vLLM instance can use total_gpu_memory (11.99GiB) x gpu_memory_utilization (0.20) = 2.40GiB\n",
      "INFO 05-26 12:09:10 worker.py:267] model weights take 0.93GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 0.04GiB.\n",
      "INFO 05-26 12:09:10 executor_base.py:111] # cuda blocks: 233, # CPU blocks: 32768\n",
      "INFO 05-26 12:09:10 executor_base.py:116] Maximum concurrency for 2060 tokens per request: 1.81x\n",
      "INFO 05-26 12:09:10 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:10<00:00,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 12:09:21 model_runner.py:1562] Graph capturing finished in 11 secs, took 0.31 GiB\n",
      "INFO 05-26 12:09:21 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 12.83 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/104 [00:00<?, ?it/s]/tmp/ipykernel_907782/3653310393.py:116: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.\n",
      "  outputs, tools_used, how_many_tool_calls = generate_batch_completion(model, tokenizer, inputs, actions_num=actions_num, lora_request=lora_request, tools=tools, temperature=0, top_p=1, max_new_tokens=768)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:09:22 scheduler.py:1754] Sequence group 22 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/104 [00:10<18:49, 10.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:09:38 scheduler.py:1754] Sequence group 377 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/104 [00:21<18:23, 10.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/104 [00:32<18:34, 11.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:09:57 scheduler.py:1754] Sequence group 1021 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/104 [00:43<18:09, 10.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 5/104 [00:58<20:18, 12.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:10:22 scheduler.py:1754] Sequence group 1642 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/104 [01:09<19:24, 11.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:10:37 scheduler.py:1754] Sequence group 1971 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/104 [01:22<19:34, 12.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/104 [01:34<19:25, 12.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:10:57 scheduler.py:1754] Sequence group 2603 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=251\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 9/104 [01:49<20:53, 13.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:11:13 scheduler.py:1754] Sequence group 2929 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=301\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 10/104 [02:02<20:20, 12.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:11:27 scheduler.py:1754] Sequence group 3218 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=351\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/104 [02:13<19:21, 12.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:11:41 scheduler.py:1754] Sequence group 3582 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=401\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/104 [02:24<18:27, 12.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 13/104 [02:34<17:24, 11.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:12:01 scheduler.py:1754] Sequence group 4173 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=451\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 14/104 [02:48<18:14, 12.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:12:16 scheduler.py:1754] Sequence group 4534 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=501\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 15/104 [03:02<18:57, 12.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 16/104 [03:11<17:00, 11.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:12:38 scheduler.py:1754] Sequence group 5175 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=551\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 17/104 [03:22<16:17, 11.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 18/104 [03:36<17:39, 12.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:12:59 scheduler.py:1754] Sequence group 5407 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=601\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 19/104 [03:46<16:05, 11.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 20/104 [03:57<16:04, 11.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:13:20 scheduler.py:1754] Sequence group 6062 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=651\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 21/104 [04:08<15:37, 11.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:13:35 scheduler.py:1754] Sequence group 6358 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=701\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 22/104 [04:21<16:07, 11.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:13:49 scheduler.py:1754] Sequence group 6702 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=751\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 23/104 [04:34<16:13, 12.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 24/104 [04:45<15:34, 11.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:14:08 scheduler.py:1754] Sequence group 7349 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=801\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 25/104 [04:56<15:15, 11.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:14:25 scheduler.py:1754] Sequence group 7678 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=851\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 26/104 [05:07<14:59, 11.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:14:39 scheduler.py:1754] Sequence group 7806 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=901\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 27/104 [05:22<15:57, 12.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 28/104 [05:36<16:15, 12.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:14:59 scheduler.py:1754] Sequence group 8431 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=951\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 29/104 [05:46<14:59, 11.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 30/104 [05:57<14:34, 11.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:15:20 scheduler.py:1754] Sequence group 8880 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1001\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 31/104 [06:08<14:11, 11.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 32/104 [06:18<13:12, 11.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:15:41 scheduler.py:1754] Sequence group 9510 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1051\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 33/104 [06:30<13:26, 11.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:15:58 scheduler.py:1754] Sequence group 9839 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1101\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 34/104 [06:42<13:26, 11.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:16:12 scheduler.py:1754] Sequence group 10175 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1151\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 35/104 [06:56<14:02, 12.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 36/104 [07:08<13:53, 12.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:16:31 scheduler.py:1754] Sequence group 10792 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1201\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 37/104 [07:23<14:30, 13.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:16:46 scheduler.py:1754] Sequence group 11110 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1251\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 38/104 [07:34<13:48, 12.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:17:01 scheduler.py:1754] Sequence group 11433 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1301\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 39/104 [07:51<14:59, 13.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 40/104 [08:01<13:28, 12.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:17:24 scheduler.py:1754] Sequence group 12084 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1351\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 41/104 [08:11<12:32, 11.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 42/104 [08:22<12:04, 11.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:17:49 scheduler.py:1754] Sequence group 12690 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1401\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 43/104 [08:32<11:08, 10.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 44/104 [08:40<10:15, 10.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:18:10 scheduler.py:1754] Sequence group 13375 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1451\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 45/104 [08:54<11:12, 11.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 46/104 [09:06<11:07, 11.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:18:29 scheduler.py:1754] Sequence group 13806 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1501\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 47/104 [09:17<10:48, 11.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:18:45 scheduler.py:1754] Sequence group 14141 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1551\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 48/104 [09:29<10:38, 11.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:18:59 scheduler.py:1754] Sequence group 14453 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1601\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 49/104 [09:43<11:09, 12.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 50/104 [09:55<11:02, 12.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:19:18 scheduler.py:1754] Sequence group 15076 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1651\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 51/104 [10:11<11:45, 13.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:19:35 scheduler.py:1754] Sequence group 15221 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1701\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 52/104 [10:23<11:16, 13.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:19:52 scheduler.py:1754] Sequence group 15541 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1751\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 53/104 [10:36<11:01, 12.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:20:07 scheduler.py:1754] Sequence group 15863 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1801\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 54/104 [10:51<11:24, 13.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 55/104 [11:02<10:29, 12.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:20:27 scheduler.py:1754] Sequence group 16510 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1851\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 56/104 [11:12<09:29, 11.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 57/104 [11:25<09:37, 12.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:20:50 scheduler.py:1754] Sequence group 17109 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1901\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 58/104 [11:37<09:25, 12.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 59/104 [11:49<09:03, 12.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:21:11 scheduler.py:1754] Sequence group 17747 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1951\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 60/104 [12:01<08:47, 11.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:21:26 scheduler.py:1754] Sequence group 18066 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2001\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 61/104 [12:14<08:46, 12.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 62/104 [12:25<08:20, 11.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:21:47 scheduler.py:1754] Sequence group 18721 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2051\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 63/104 [12:34<07:34, 11.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:22:03 scheduler.py:1754] Sequence group 19064 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2101\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 64/104 [12:49<08:16, 12.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 65/104 [12:59<07:34, 11.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:22:25 scheduler.py:1754] Sequence group 19665 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2151\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 66/104 [13:12<07:35, 12.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:22:39 scheduler.py:1754] Sequence group 20014 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2201\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 67/104 [13:25<07:30, 12.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 68/104 [13:36<07:14, 12.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:23:01 scheduler.py:1754] Sequence group 20437 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2251\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 69/104 [13:48<07:01, 12.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 70/104 [13:57<06:14, 11.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:23:22 scheduler.py:1754] Sequence group 20916 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2301\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 71/104 [14:06<05:47, 10.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 72/104 [14:20<06:03, 11.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:23:43 scheduler.py:1754] Sequence group 21550 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2351\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 73/104 [14:31<05:49, 11.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:23:59 scheduler.py:1754] Sequence group 21866 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2401\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 74/104 [14:46<06:16, 12.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:24:14 scheduler.py:1754] Sequence group 22197 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2451\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 75/104 [14:59<06:04, 12.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 76/104 [15:11<05:47, 12.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:24:35 scheduler.py:1754] Sequence group 22847 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2501\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 77/104 [15:21<05:14, 11.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 78/104 [15:32<04:55, 11.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:24:55 scheduler.py:1754] Sequence group 23461 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2551\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 79/104 [15:44<04:50, 11.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:25:10 scheduler.py:1754] Sequence group 23785 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2601\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 80/104 [15:57<04:46, 11.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 81/104 [16:08<04:31, 11.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:25:31 scheduler.py:1754] Sequence group 24436 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2651\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 82/104 [16:20<04:19, 11.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 83/104 [16:28<03:46, 10.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:25:52 scheduler.py:1754] Sequence group 25061 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2701\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 84/104 [16:40<03:41, 11.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:26:08 scheduler.py:1754] Sequence group 25200 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2751\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 85/104 [16:52<03:38, 11.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 86/104 [17:04<03:27, 11.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:26:32 scheduler.py:1754] Sequence group 25840 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2801\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 87/104 [17:16<03:20, 11.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 12:26:51 scheduler.py:1754] Sequence group 26386 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 88/104 [17:29<03:14, 12.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 89/104 [17:42<03:02, 12.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:27:06 scheduler.py:1754] Sequence group 26809 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2901\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 90/104 [17:55<02:54, 12.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 91/104 [18:06<02:38, 12.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:27:29 scheduler.py:1754] Sequence group 27443 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2951\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 92/104 [18:15<02:14, 11.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 93/104 [18:27<02:05, 11.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:27:49 scheduler.py:1754] Sequence group 28053 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3001\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 94/104 [18:38<01:52, 11.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:28:06 scheduler.py:1754] Sequence group 28380 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3051\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 95/104 [18:52<01:47, 11.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:28:20 scheduler.py:1754] Sequence group 28530 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3101\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 96/104 [19:04<01:36, 12.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 97/104 [19:13<01:18, 11.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:28:40 scheduler.py:1754] Sequence group 28962 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3151\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 98/104 [19:27<01:12, 12.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:28:55 scheduler.py:1754] Sequence group 29286 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3201\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 99/104 [19:41<01:02, 12.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 100/104 [19:53<00:49, 12.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:29:16 scheduler.py:1754] Sequence group 29948 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3251\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 101/104 [20:02<00:34, 11.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 102/104 [20:12<00:21, 10.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 12:29:40 scheduler.py:1754] Sequence group 30578 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3301\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 103/104 [20:27<00:12, 12.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [20:35<00:00, 11.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score: 0.0930\n",
      "Average tools used: 0.3812\n",
      "Average calls per sample: 1.0107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from pretrained peft model\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "\n",
    "base_model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "inference_engine = LLM(\n",
    "    model=base_model_name,\n",
    "    enable_lora=True,\n",
    "    max_lora_rank=64,\n",
    "    max_loras=1,\n",
    "    gpu_memory_utilization=0.2,\n",
    "    # enable_prefix_caching=True,\n",
    "    swap_space=6,\n",
    "    scheduling_policy=\"fcfs\",\n",
    "    dtype=torch.bfloat16,\n",
    "    max_model_len=2060,\n",
    "    # enable_sleep_mode=True,\n",
    "    )\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "avg_bleu, tools_used_avg, calls_per_sample_avg = evaluate_model(inference_engine, tokenizer, dataloader, actions_num=4, lora_request=None, tools=TOOLS)\n",
    "print(f\"Average BLEU score: {avg_bleu:.4f}\")\n",
    "print(f\"Average tools used: {tools_used_avg:.4f}\")\n",
    "print(f\"Average calls per sample: {calls_per_sample_avg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59db9f6",
   "metadata": {},
   "source": [
    "## Model with SFT without tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "042408ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-23 21:17:03,093] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-23 21:17:04 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 05-23 21:17:09 config.py:549] This model supports multiple tasks: {'embed', 'score', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 05-23 21:17:09 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='Qwen/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2060, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-0.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 05-23 21:17:09 interface.py:304] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 05-23 21:17:09 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 05-23 21:17:09 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-0.5B-Instruct...\n",
      "INFO 05-23 21:17:10 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 05-23 21:17:10 weight_utils.py:304] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.30it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-23 21:17:11 model_runner.py:1115] Loading model weights took 0.9277 GB\n",
      "INFO 05-23 21:17:11 punica_selector.py:18] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-23 21:17:12 worker.py:267] Memory profiling takes 0.94 seconds\n",
      "INFO 05-23 21:17:12 worker.py:267] the current vLLM instance can use total_gpu_memory (11.99GiB) x gpu_memory_utilization (0.20) = 2.40GiB\n",
      "INFO 05-23 21:17:12 worker.py:267] model weights take 0.93GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 0.04GiB.\n",
      "INFO 05-23 21:17:12 executor_base.py:111] # cuda blocks: 233, # CPU blocks: 32768\n",
      "INFO 05-23 21:17:12 executor_base.py:116] Maximum concurrency for 2060 tokens per request: 1.81x\n",
      "INFO 05-23 21:17:12 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:10<00:00,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-23 21:17:23 model_runner.py:1562] Graph capturing finished in 11 secs, took 0.31 GiB\n",
      "INFO 05-23 21:17:23 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 12.13 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/104 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 21:17:23 tokenizer.py:264] No tokenizer found in models/sft_base_qwen_no_tools, using base model tokenizer instead. (Exception: Unrecognized model in models/sft_base_qwen_no_tools. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_320289/431794125.py:113: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.\n",
      "  outputs, tools_used, how_many_tool_calls = generate_batch_completion(model, tokenizer, inputs, actions_num=actions_num, lora_request=lora_request, tools=tools, temperature=0, top_p=1, max_new_tokens=768)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 21:17:24 scheduler.py:1754] Sequence group 23 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/104 [00:21<37:28, 21.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/104 [00:44<38:02, 22.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/104 [01:05<36:47, 21.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/104 [01:26<35:45, 21.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 21:18:51 scheduler.py:1754] Sequence group 1312 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 5/104 [01:47<35:18, 21.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/104 [02:14<26:03, 16.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/104 [02:35<27:54, 17.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 21:19:59 scheduler.py:1754] Sequence group 2591 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 9/104 [02:57<30:03, 18.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 10/104 [03:06<24:54, 15.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/104 [03:27<27:07, 17.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/104 [03:50<29:08, 19.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 21:21:16 scheduler.py:1754] Sequence group 3897 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 13/104 [04:12<30:26, 20.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 14/104 [04:34<31:01, 20.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 15/104 [05:04<34:44, 23.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 16/104 [05:26<33:48, 23.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 21:22:50 scheduler.py:1754] Sequence group 5141 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 17/104 [05:45<31:32, 21.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 19/104 [06:10<23:03, 16.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 20/104 [06:32<25:04, 17.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 21:23:58 scheduler.py:1754] Sequence group 6461 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 21/104 [06:36<19:09, 13.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 22/104 [07:00<23:04, 16.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 23/104 [07:22<24:54, 18.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 24/104 [07:45<26:26, 19.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 21:25:10 scheduler.py:1754] Sequence group 7718 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=301\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 25/104 [08:09<27:29, 20.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 26/104 [08:33<28:43, 22.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 27/104 [08:56<28:39, 22.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 28/104 [09:21<29:05, 22.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 21:26:46 scheduler.py:1754] Sequence group 9000 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=351\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 29/104 [09:42<28:10, 22.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 30/104 [10:03<27:17, 22.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 31/104 [10:25<26:43, 21.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 32/104 [10:34<21:46, 18.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 21:28:00 scheduler.py:1754] Sequence group 10296 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=401\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 33/104 [10:57<22:56, 19.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 34/104 [11:06<19:17, 16.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 35/104 [11:27<20:21, 17.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 36/104 [11:35<16:45, 14.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 37/104 [11:57<18:59, 17.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 21:29:22 scheduler.py:1754] Sequence group 11871 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=451\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 38/104 [12:20<20:31, 18.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 39/104 [12:35<19:13, 17.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 40/104 [12:55<19:35, 18.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 41/104 [13:08<17:27, 16.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 21:30:33 scheduler.py:1754] Sequence group 13174 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 42/104 [13:12<13:24, 12.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 43/104 [13:35<16:22, 16.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 44/104 [13:57<17:50, 17.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 45/104 [14:21<19:13, 19.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 46/104 [14:29<15:36, 16.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 21:31:54 scheduler.py:1754] Sequence group 14756 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=551\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 47/104 [14:51<16:51, 17.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 50/104 [15:08<08:38,  9.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 21:32:38 scheduler.py:1754] Sequence group 16034 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=601\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 51/104 [15:33<12:38, 14.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 52/104 [15:58<15:08, 17.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 53/104 [16:23<16:45, 19.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 54/104 [16:46<17:14, 20.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 21:34:10 scheduler.py:1754] Sequence group 17300 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=651\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 55/104 [17:08<17:08, 21.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 56/104 [17:34<18:03, 22.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 57/104 [17:39<13:30, 17.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 21:35:04 scheduler.py:1754] Sequence group 18284 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=701\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 58/104 [17:59<13:58, 18.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 59/104 [18:07<11:23, 15.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 60/104 [18:29<12:29, 17.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 61/104 [18:52<13:39, 19.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 21:36:18 scheduler.py:1754] Sequence group 19562 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=751\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 63/104 [19:19<10:27, 15.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 64/104 [19:38<10:51, 16.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 65/104 [20:02<12:10, 18.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 21:37:28 scheduler.py:1754] Sequence group 20858 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=801\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 66/104 [20:11<09:59, 15.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 67/104 [20:32<10:45, 17.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 68/104 [20:54<11:08, 18.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 69/104 [21:17<11:45, 20.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 70/104 [21:39<11:38, 20.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 21:39:04 scheduler.py:1754] Sequence group 22439 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=851\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 71/104 [22:01<11:30, 20.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 72/104 [22:23<11:22, 21.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 73/104 [22:44<10:59, 21.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 74/104 [23:07<10:57, 21.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 21:40:34 scheduler.py:1754] Sequence group 23743 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=901\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 75/104 [23:30<10:43, 22.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 76/104 [23:52<10:17, 22.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 78/104 [24:17<07:06, 16.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-23 21:41:53 scheduler.py:1754] Sequence group 25248 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 79/104 [24:30<06:26, 15.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 80/104 [24:55<07:17, 18.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 81/104 [25:20<07:46, 20.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 82/104 [25:43<07:43, 21.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 83/104 [26:06<07:32, 21.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 21:43:30 scheduler.py:1754] Sequence group 26588 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1001\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 84/104 [26:16<06:02, 18.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 85/104 [26:26<04:59, 15.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 86/104 [26:47<05:14, 17.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 87/104 [27:11<05:26, 19.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 21:44:37 scheduler.py:1754] Sequence group 27899 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1051\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 88/104 [27:31<05:15, 19.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 89/104 [27:54<05:05, 20.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 90/104 [28:19<05:07, 21.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 91/104 [28:44<04:55, 22.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 21:46:08 scheduler.py:1754] Sequence group 29147 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1101\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 92/104 [29:04<04:25, 22.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 93/104 [29:27<04:03, 22.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 94/104 [29:36<03:01, 18.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 95/104 [30:03<03:08, 20.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 21:47:28 scheduler.py:1754] Sequence group 30425 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1151\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 96/104 [30:24<02:48, 21.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 97/104 [30:35<02:06, 18.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 99/104 [31:01<01:12, 14.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 21:48:27 scheduler.py:1754] Sequence group 31741 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1201\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 100/104 [31:19<01:03, 15.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 101/104 [31:43<00:54, 18.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 103/104 [32:09<00:14, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [32:16<00:00, 18.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score: 8.2227\n",
      "Average tools used: 0.0000\n",
      "Average calls per sample: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from pretrained peft model\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "\n",
    "# def load_model(model_name_or_path, peft_model_id):\n",
    "#     # Load the base model\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\", torch_dtype='auto')\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "#     # Load the PEFT model\n",
    "#     peft_model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "    \n",
    "#     return peft_model, tokenizer\n",
    "\n",
    "\n",
    "vllm_lora_adapter = 'models/sft_base_qwen_no_tools'\n",
    "base_model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "inference_engine = LLM(\n",
    "    model=base_model_name,\n",
    "    enable_lora=True,\n",
    "    max_lora_rank=64,\n",
    "    max_loras=1,\n",
    "    gpu_memory_utilization=0.2,\n",
    "    # enable_prefix_caching=True,\n",
    "    swap_space=6,\n",
    "    scheduling_policy=\"fcfs\",\n",
    "    dtype=torch.bfloat16,\n",
    "    max_model_len=2060,\n",
    "    # enable_sleep_mode=True,\n",
    "    )\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "avg_bleu, tools_used_avg, calls_per_sample_avg = evaluate_model(inference_engine, tokenizer, dataloader, actions_num=4, lora_request=LoRARequest('adapter', 1, vllm_lora_adapter), tools=TOOLS)\n",
    "print(f\"Average BLEU score: {avg_bleu:.4f}\")\n",
    "print(f\"Average tools used: {tools_used_avg:.4f}\")\n",
    "print(f\"Average calls per sample: {calls_per_sample_avg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e573b92",
   "metadata": {},
   "source": [
    "### Model with SFT with tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7707bb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-21 14:53:23,603] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-21 14:53:24 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 05-21 14:53:28 config.py:549] This model supports multiple tasks: {'generate', 'reward', 'classify', 'embed', 'score'}. Defaulting to 'generate'.\n",
      "INFO 05-21 14:53:28 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='Qwen/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-0.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 05-21 14:53:29 interface.py:304] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 05-21 14:53:29 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 05-21 14:53:29 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-0.5B-Instruct...\n",
      "INFO 05-21 14:53:29 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 05-21 14:53:30 weight_utils.py:304] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.57it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-21 14:53:30 model_runner.py:1115] Loading model weights took 0.9277 GB\n",
      "INFO 05-21 14:53:30 punica_selector.py:18] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-21 14:53:31 worker.py:267] Memory profiling takes 0.88 seconds\n",
      "INFO 05-21 14:53:31 worker.py:267] the current vLLM instance can use total_gpu_memory (11.99GiB) x gpu_memory_utilization (0.20) = 2.40GiB\n",
      "INFO 05-21 14:53:31 worker.py:267] model weights take 0.93GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 0.04GiB.\n",
      "INFO 05-21 14:53:31 executor_base.py:111] # cuda blocks: 233, # CPU blocks: 32768\n",
      "INFO 05-21 14:53:31 executor_base.py:116] Maximum concurrency for 768 tokens per request: 4.85x\n",
      "INFO 05-21 14:53:32 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:11<00:00,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-21 14:53:43 model_runner.py:1562] Graph capturing finished in 11 secs, took 0.31 GiB\n",
      "INFO 05-21 14:53:43 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 12.69 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/104 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 14:53:43 tokenizer.py:264] No tokenizer found in models/sft_base_qwen2, using base model tokenizer instead. (Exception: Can't load tokenizer for 'models/sft_base_qwen2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'models/sft_base_qwen2' is the correct path to a directory containing all relevant files for a Qwen2TokenizerFast tokenizer.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_123978/4290022123.py:113: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.\n",
      "  outputs, tools_used, how_many_tool_calls = generate_batch_completion(model, tokenizer, inputs, actions_num=actions_num, lora_request=lora_request, tools=tools, temperature=0, top_p=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 14:53:44 scheduler.py:1754] Sequence group 41 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/104 [00:07<13:43,  8.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/104 [00:18<16:04,  9.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/104 [00:31<18:27, 10.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/104 [00:45<20:24, 12.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 5/104 [00:58<20:37, 12.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 14:54:45 scheduler.py:1754] Sequence group 1743 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/104 [01:07<18:44, 11.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/104 [01:27<16:29, 10.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 9/104 [01:40<17:57, 11.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 10/104 [01:50<16:53, 10.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 14:55:39 scheduler.py:1754] Sequence group 3385 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/104 [02:03<17:35, 11.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/104 [02:12<16:18, 10.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 13/104 [02:23<16:22, 10.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 14/104 [02:34<16:34, 11.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 15/104 [02:49<18:00, 12.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 16/104 [03:02<18:02, 12.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 14:56:46 scheduler.py:1754] Sequence group 5140 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 17/104 [03:13<17:30, 12.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 18/104 [03:26<17:40, 12.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 19/104 [03:36<16:13, 11.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 21/104 [03:53<13:49,  9.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 14:57:40 scheduler.py:1754] Sequence group 6843 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 22/104 [04:04<13:45, 10.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 23/104 [04:16<14:22, 10.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 24/104 [04:30<15:31, 11.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 25/104 [04:42<15:39, 11.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 26/104 [04:53<14:55, 11.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 14:58:40 scheduler.py:1754] Sequence group 8485 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=251\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 27/104 [05:02<14:01, 10.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 28/104 [05:16<14:45, 11.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 29/104 [05:28<14:49, 11.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 30/104 [05:40<14:37, 11.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 31/104 [05:53<14:52, 12.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 14:59:37 scheduler.py:1754] Sequence group 9940 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=301\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 32/104 [06:05<14:42, 12.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 33/104 [06:19<15:03, 12.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 35/104 [06:40<13:03, 11.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 36/104 [06:50<12:33, 11.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:00:35 scheduler.py:1754] Sequence group 11579 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=351\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 37/104 [07:02<12:37, 11.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 38/104 [07:12<11:46, 10.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 39/104 [07:22<11:23, 10.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 40/104 [07:36<12:32, 11.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 41/104 [07:48<12:29, 11.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:01:32 scheduler.py:1754] Sequence group 13140 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=401\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 44/104 [08:17<10:03, 10.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 45/104 [08:30<10:36, 10.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:02:19 scheduler.py:1754] Sequence group 14588 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=451\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 47/104 [08:50<09:41, 10.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 48/104 [09:02<09:58, 10.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 49/104 [09:15<10:37, 11.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 50/104 [09:28<10:38, 11.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 51/104 [09:41<10:44, 12.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:03:25 scheduler.py:1754] Sequence group 16381 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=501\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 52/104 [09:54<10:53, 12.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 53/104 [10:08<11:01, 12.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 55/104 [10:29<09:17, 11.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:04:16 scheduler.py:1754] Sequence group 17743 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=551\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 56/104 [10:42<09:36, 12.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:04:26 scheduler.py:1091] Input prompt (2049 tokens) is too long and exceeds limit of 768\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-21 15:04:28 scheduler.py:1091] Input prompt (2050 tokens) is too long and exceeds limit of 768\n",
      "WARNING 05-21 15:04:31 scheduler.py:1091] Input prompt (2050 tokens) is too long and exceeds limit of 768\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-21 15:04:34 scheduler.py:1091] Input prompt (2050 tokens) is too long and exceeds limit of 768\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-21 15:04:37 scheduler.py:1091] Input prompt (2050 tokens) is too long and exceeds limit of 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 57/104 [10:54<09:26, 12.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 59/104 [11:14<08:07, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 60/104 [11:25<07:58, 10.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 61/104 [11:35<07:34, 10.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:05:19 scheduler.py:1754] Sequence group 19559 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=601\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 62/104 [11:47<07:42, 11.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 63/104 [11:58<07:27, 10.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:05:42 scheduler.py:1091] Input prompt (1514 tokens) is too long and exceeds limit of 768\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-21 15:05:43 scheduler.py:1091] Input prompt (1515 tokens) is too long and exceeds limit of 768\n",
      "WARNING 05-21 15:05:46 scheduler.py:1091] Input prompt (1515 tokens) is too long and exceeds limit of 768\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-21 15:05:50 scheduler.py:1091] Input prompt (1515 tokens) is too long and exceeds limit of 768\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-21 15:05:52 scheduler.py:1091] Input prompt (1515 tokens) is too long and exceeds limit of 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 64/104 [12:09<07:21, 11.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 65/104 [12:23<07:43, 11.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:06:11 scheduler.py:1754] Sequence group 20970 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=651\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 66/104 [12:36<07:43, 12.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 67/104 [12:48<07:26, 12.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 68/104 [12:59<07:05, 11.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 69/104 [13:10<06:47, 11.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 70/104 [13:23<06:45, 11.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:07:09 scheduler.py:1754] Sequence group 22497 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=701\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 71/104 [13:35<06:40, 12.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 72/104 [13:45<06:03, 11.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 73/104 [13:57<05:56, 11.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 74/104 [14:09<05:54, 11.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 75/104 [14:22<05:46, 11.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 76/104 [14:34<05:37, 12.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:08:18 scheduler.py:1754] Sequence group 24340 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=751\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 77/104 [14:46<05:23, 11.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 78/104 [14:55<04:46, 11.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 79/104 [15:07<04:47, 11.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 80/104 [15:17<04:24, 11.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:09:02 scheduler.py:1754] Sequence group 25661 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=801\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 81/104 [15:29<04:20, 11.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 82/104 [15:40<04:06, 11.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 83/104 [15:52<03:57, 11.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 84/104 [16:03<03:45, 11.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 86/104 [16:22<03:05, 10.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:10:06 scheduler.py:1754] Sequence group 27558 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=851\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 87/104 [16:33<02:56, 10.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 88/104 [16:47<03:02, 11.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 89/104 [16:59<02:53, 11.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 90/104 [17:11<02:43, 11.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 91/104 [17:23<02:33, 11.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:11:07 scheduler.py:1754] Sequence group 29159 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=901\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 93/104 [17:43<01:58, 10.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 94/104 [17:57<01:57, 11.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:11:42 scheduler.py:1091] Input prompt (834 tokens) is too long and exceeds limit of 768\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-21 15:11:44 scheduler.py:1091] Input prompt (835 tokens) is too long and exceeds limit of 768\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-21 15:11:49 scheduler.py:1091] Input prompt (835 tokens) is too long and exceeds limit of 768\n",
      "WARNING 05-21 15:11:52 scheduler.py:1091] Input prompt (835 tokens) is too long and exceeds limit of 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 96/104 [18:19<01:27, 10.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:12:03 scheduler.py:1754] Sequence group 30760 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=951\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 97/104 [18:31<01:20, 11.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 98/104 [18:43<01:09, 11.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 99/104 [18:55<00:58, 11.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 100/104 [19:05<00:45, 11.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 101/104 [19:15<00:32, 10.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:13:00 scheduler.py:1754] Sequence group 32411 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1001\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 103/104 [19:35<00:10, 10.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [19:43<00:00, 11.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score: 13.2998\n",
      "Average tools used: 0.9025\n",
      "Average calls per sample: 2.1136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from pretrained peft model\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "\n",
    "# def load_model(model_name_or_path, peft_model_id):\n",
    "#     # Load the base model\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\", torch_dtype='auto')\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "#     # Load the PEFT model\n",
    "#     peft_model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "    \n",
    "#     return peft_model, tokenizer\n",
    "\n",
    "\n",
    "vllm_lora_adapter = 'models/sft_base_qwen2'\n",
    "base_model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "inference_engine = LLM(\n",
    "    model=base_model_name,\n",
    "    enable_lora=True,\n",
    "    max_lora_rank=64,\n",
    "    max_loras=1,\n",
    "    gpu_memory_utilization=0.2,\n",
    "    # enable_prefix_caching=True,\n",
    "    swap_space=6,\n",
    "    scheduling_policy=\"fcfs\",\n",
    "    dtype=torch.bfloat16,\n",
    "    max_model_len=2060,\n",
    "    # enable_sleep_mode=True,\n",
    "    )\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "avg_bleu, tools_used_avg, calls_per_sample_avg = evaluate_model(inference_engine, tokenizer, dataloader, actions_num=4, lora_request=LoRARequest('adapter', 1, vllm_lora_adapter), tools=TOOLS)\n",
    "print(f\"Average BLEU score: {avg_bleu:.4f}\")\n",
    "print(f\"Average tools used: {tools_used_avg:.4f}\")\n",
    "print(f\"Average calls per sample: {calls_per_sample_avg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7da6b6",
   "metadata": {},
   "source": [
    "## Model with SFT with tools + RL with tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "498c82ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-26 10:49:48,828] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 10:49:49 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 05-26 10:49:54 config.py:549] This model supports multiple tasks: {'generate', 'reward', 'score', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 05-26 10:49:54 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='Qwen/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-0.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 05-26 10:49:54 interface.py:304] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 05-26 10:49:54 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 05-26 10:49:55 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-0.5B-Instruct...\n",
      "INFO 05-26 10:49:55 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 05-26 10:49:55 weight_utils.py:304] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.04s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 10:49:57 model_runner.py:1115] Loading model weights took 0.9277 GB\n",
      "INFO 05-26 10:49:57 punica_selector.py:18] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 10:49:59 worker.py:267] Memory profiling takes 1.80 seconds\n",
      "INFO 05-26 10:49:59 worker.py:267] the current vLLM instance can use total_gpu_memory (11.99GiB) x gpu_memory_utilization (0.20) = 2.40GiB\n",
      "INFO 05-26 10:49:59 worker.py:267] model weights take 0.93GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 0.04GiB.\n",
      "INFO 05-26 10:49:59 executor_base.py:111] # cuda blocks: 233, # CPU blocks: 32768\n",
      "INFO 05-26 10:49:59 executor_base.py:116] Maximum concurrency for 2048 tokens per request: 1.82x\n",
      "INFO 05-26 10:50:01 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:11<00:00,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 10:50:12 model_runner.py:1562] Graph capturing finished in 11 secs, took 0.31 GiB\n",
      "INFO 05-26 10:50:12 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 14.55 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/139 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 10:50:12 tokenizer.py:264] No tokenizer found in models/best_policy_model11, using base model tokenizer instead. (Exception: Unrecognized model in models/best_policy_model11. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_694212/431794125.py:113: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.\n",
      "  outputs, tools_used, how_many_tool_calls = generate_batch_completion(model, tokenizer, inputs, actions_num=actions_num, lora_request=lora_request, tools=tools, temperature=0, top_p=1, max_new_tokens=768)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 10:50:13 scheduler.py:1754] Sequence group 20 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/139 [00:13<31:11, 13.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2/139 [00:25<29:24, 12.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/139 [00:43<33:31, 14.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 10:51:08 scheduler.py:1754] Sequence group 948 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4/139 [01:02<37:02, 16.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 5/139 [01:14<33:47, 15.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 6/139 [01:27<31:54, 14.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 7/139 [01:45<33:43, 15.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 8/139 [02:03<35:24, 16.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 10:52:16 scheduler.py:1754] Sequence group 1961 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 9/139 [02:21<36:45, 16.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 10/139 [02:34<33:46, 15.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 11/139 [02:53<35:27, 16.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 10:53:13 scheduler.py:1754] Sequence group 2849 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 12/139 [03:10<35:34, 16.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 13/139 [03:25<33:56, 16.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 14/139 [03:42<34:22, 16.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 15/139 [03:58<33:30, 16.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 10:54:11 scheduler.py:1754] Sequence group 3641 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 16/139 [04:16<34:22, 16.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 17/139 [04:32<33:39, 16.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 18/139 [04:45<31:12, 15.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 10:55:03 scheduler.py:1754] Sequence group 4508 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 19/139 [05:02<32:09, 16.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 20/139 [05:19<32:23, 16.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 21/139 [05:34<31:05, 15.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 10:55:58 scheduler.py:1754] Sequence group 5264 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 22/139 [05:51<31:54, 16.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 23/139 [06:11<33:24, 17.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 24/139 [06:23<30:16, 15.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 25/139 [06:38<29:36, 15.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 10:56:51 scheduler.py:1754] Sequence group 6020 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 26/139 [06:52<28:34, 15.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 27/139 [07:13<31:22, 16.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 28/139 [07:26<28:56, 15.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 10:57:46 scheduler.py:1754] Sequence group 6942 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 29/139 [07:39<27:00, 14.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 30/139 [07:52<25:53, 14.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 31/139 [08:06<25:41, 14.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 32/139 [08:22<26:28, 14.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 10:58:44 scheduler.py:1754] Sequence group 7907 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 33/139 [08:35<24:56, 14.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 34/139 [08:50<25:20, 14.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 35/139 [09:07<26:38, 15.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 36/139 [09:23<26:45, 15.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 10:59:45 scheduler.py:1754] Sequence group 8859 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 37/139 [09:43<28:24, 16.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 38/139 [09:59<27:46, 16.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 39/139 [10:14<26:59, 16.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:00:36 scheduler.py:1754] Sequence group 9576 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 40/139 [10:30<26:37, 16.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 41/139 [10:46<26:09, 16.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 42/139 [11:01<25:36, 15.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:01:27 scheduler.py:1754] Sequence group 10304 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 43/139 [11:20<26:48, 16.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 44/139 [11:36<25:57, 16.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 45/139 [11:52<25:25, 16.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 46/139 [12:08<25:07, 16.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:02:30 scheduler.py:1754] Sequence group 11266 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 47/139 [12:25<25:10, 16.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 48/139 [12:38<23:36, 15.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 49/139 [12:52<22:17, 14.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 50/139 [13:09<23:21, 15.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:03:31 scheduler.py:1754] Sequence group 12220 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 51/139 [13:26<23:21, 15.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 52/139 [13:42<23:25, 16.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 53/139 [13:58<22:44, 15.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 54/139 [14:11<21:13, 14.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:04:25 scheduler.py:1754] Sequence group 13050 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 55/139 [14:27<21:28, 15.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 56/139 [14:38<19:43, 14.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 57/139 [14:56<20:43, 15.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:05:17 scheduler.py:1754] Sequence group 13901 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 58/139 [15:09<19:42, 14.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 59/139 [15:26<20:35, 15.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 60/139 [15:46<22:08, 16.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:06:09 scheduler.py:1754] Sequence group 14607 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 61/139 [16:06<23:07, 17.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 62/139 [16:22<22:08, 17.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 63/139 [16:36<20:22, 16.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:06:58 scheduler.py:1754] Sequence group 15344 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 64/139 [16:50<19:29, 15.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 65/139 [17:12<21:25, 17.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 66/139 [17:24<19:08, 15.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 67/139 [17:36<17:44, 14.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:08:00 scheduler.py:1754] Sequence group 16296 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 68/139 [17:56<19:11, 16.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 69/139 [18:09<17:52, 15.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 70/139 [18:26<18:07, 15.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:08:54 scheduler.py:1754] Sequence group 17026 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 71/139 [18:45<19:06, 16.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 72/139 [19:00<18:14, 16.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 73/139 [19:14<17:01, 15.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 74/139 [19:30<17:01, 15.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 75/139 [19:47<17:15, 16.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:10:01 scheduler.py:1091] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:10:02 scheduler.py:1091] Input prompt (2050 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-26 11:10:03 scheduler.py:1091] Input prompt (2050 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-26 11:10:05 scheduler.py:1091] Input prompt (2050 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-26 11:10:07 scheduler.py:1754] Sequence group 18206 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1051\n",
      "WARNING 05-26 11:10:11 scheduler.py:1091] Input prompt (2050 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 76/139 [20:00<15:53, 15.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 77/139 [20:19<16:50, 16.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 78/139 [20:38<17:21, 17.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 79/139 [20:51<15:49, 15.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:11:10 scheduler.py:1754] Sequence group 19164 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 80/139 [21:08<15:48, 16.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 81/139 [21:19<14:03, 14.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 82/139 [21:34<14:10, 14.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 83/139 [21:47<13:24, 14.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 84/139 [22:00<12:41, 13.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:12:14 scheduler.py:1754] Sequence group 20222 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1151\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 85/139 [22:17<13:19, 14.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 86/139 [22:33<13:18, 15.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 87/139 [22:53<14:18, 16.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:13:08 scheduler.py:1754] Sequence group 20986 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 88/139 [23:07<13:25, 15.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 89/139 [23:18<12:01, 14.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 90/139 [23:40<13:35, 16.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 91/139 [23:52<12:19, 15.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:14:06 scheduler.py:1754] Sequence group 21881 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 92/139 [24:08<12:10, 15.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 93/139 [24:21<11:10, 14.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 94/139 [24:35<11:01, 14.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 95/139 [24:47<10:10, 13.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:15:08 scheduler.py:1754] Sequence group 23008 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 96/139 [25:03<10:21, 14.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 97/139 [25:18<10:16, 14.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 98/139 [25:35<10:23, 15.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:16:01 scheduler.py:1754] Sequence group 23750 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 99/139 [25:50<10:11, 15.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 100/139 [26:05<09:43, 14.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 101/139 [26:21<09:42, 15.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 102/139 [26:36<09:27, 15.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:16:52 scheduler.py:1754] Sequence group 24617 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 103/139 [26:49<08:46, 14.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 104/139 [27:01<08:03, 13.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 105/139 [27:19<08:32, 15.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:17:37 scheduler.py:1754] Sequence group 25375 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1451\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 106/139 [27:39<09:04, 16.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 107/139 [27:52<08:15, 15.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 108/139 [28:05<07:39, 14.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 109/139 [28:21<07:34, 15.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:18:34 scheduler.py:1754] Sequence group 26180 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 110/139 [28:34<07:01, 14.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 111/139 [28:51<07:07, 15.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 112/139 [29:06<06:50, 15.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:19:30 scheduler.py:1754] Sequence group 27091 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 113/139 [29:22<06:37, 15.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 114/139 [29:38<06:29, 15.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 115/139 [29:54<06:16, 15.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:20:17 scheduler.py:1754] Sequence group 27806 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 116/139 [30:14<06:31, 17.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 117/139 [30:27<05:49, 15.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 118/139 [30:45<05:43, 16.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 119/139 [31:01<05:24, 16.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:21:15 scheduler.py:1754] Sequence group 28636 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 120/139 [31:18<05:13, 16.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 121/139 [31:33<04:48, 16.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 122/139 [31:44<04:09, 14.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 123/139 [31:57<03:45, 14.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:22:11 scheduler.py:1754] Sequence group 29582 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 124/139 [32:13<03:41, 14.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 125/139 [32:25<03:12, 13.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 126/139 [32:46<03:29, 16.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:23:06 scheduler.py:1754] Sequence group 30447 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 127/139 [33:04<03:16, 16.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 128/139 [33:20<02:59, 16.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 129/139 [33:36<02:43, 16.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:23:59 scheduler.py:1754] Sequence group 31195 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 130/139 [33:50<02:20, 15.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 131/139 [34:03<01:58, 14.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 132/139 [34:18<01:44, 14.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 133/139 [34:38<01:39, 16.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:24:59 scheduler.py:1754] Sequence group 32135 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 134/139 [34:53<01:19, 15.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 135/139 [35:10<01:05, 16.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 136/139 [35:22<00:45, 15.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:25:47 scheduler.py:1754] Sequence group 32878 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 137/139 [35:38<00:30, 15.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 138/139 [35:54<00:15, 15.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [35:56<00:00, 15.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score: 21.5450\n",
      "Average tools used: 0.9998\n",
      "Average calls per sample: 3.7474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from pretrained peft model\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "\n",
    "vllm_lora_adapter = 'models/best_policy_model11'\n",
    "base_model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "inference_engine = LLM(\n",
    "    model=base_model_name,\n",
    "    enable_lora=True,\n",
    "    max_lora_rank=64,\n",
    "    max_loras=1,\n",
    "    gpu_memory_utilization=0.2,\n",
    "    # enable_prefix_caching=True,\n",
    "    swap_space=6,\n",
    "    scheduling_policy=\"fcfs\",\n",
    "    dtype=torch.bfloat16,\n",
    "    max_model_len=2048,\n",
    "    # enable_sleep_mode=True,\n",
    "    )\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=48, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "avg_bleu, tools_used_avg, calls_per_sample_avg = evaluate_model(inference_engine, tokenizer, dataloader, actions_num=4, lora_request=LoRARequest('adapter', 1, vllm_lora_adapter), tools=TOOLS)\n",
    "print(f\"Average BLEU score: {avg_bleu:.4f}\")\n",
    "print(f\"Average tools used: {tools_used_avg:.4f}\")\n",
    "print(f\"Average calls per sample: {calls_per_sample_avg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bbeaf5",
   "metadata": {},
   "source": [
    "### Model with SFT no tools + RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9db88073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-26 11:29:47,133] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 11:29:49 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 05-26 11:29:53 config.py:549] This model supports multiple tasks: {'embed', 'generate', 'score', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 05-26 11:29:53 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='Qwen/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-0.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 05-26 11:29:54 interface.py:304] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 05-26 11:29:54 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 05-26 11:29:54 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-0.5B-Instruct...\n",
      "INFO 05-26 11:29:54 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 05-26 11:29:55 weight_utils.py:304] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.44s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 11:29:56 model_runner.py:1115] Loading model weights took 0.9277 GB\n",
      "INFO 05-26 11:29:56 punica_selector.py:18] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 11:29:58 worker.py:267] Memory profiling takes 1.34 seconds\n",
      "INFO 05-26 11:29:58 worker.py:267] the current vLLM instance can use total_gpu_memory (11.99GiB) x gpu_memory_utilization (0.20) = 2.40GiB\n",
      "INFO 05-26 11:29:58 worker.py:267] model weights take 0.93GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 0.04GiB.\n",
      "INFO 05-26 11:29:58 executor_base.py:111] # cuda blocks: 233, # CPU blocks: 32768\n",
      "INFO 05-26 11:29:58 executor_base.py:116] Maximum concurrency for 2048 tokens per request: 1.82x\n",
      "INFO 05-26 11:29:59 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:11<00:00,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 11:30:10 model_runner.py:1562] Graph capturing finished in 11 secs, took 0.31 GiB\n",
      "INFO 05-26 11:30:10 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 13.11 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/139 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 11:30:10 tokenizer.py:264] No tokenizer found in models/best_policy_model10, using base model tokenizer instead. (Exception: Unrecognized model in models/best_policy_model10. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_900495/3653310393.py:116: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.\n",
      "  outputs, tools_used, how_many_tool_calls = generate_batch_completion(model, tokenizer, inputs, actions_num=actions_num, lora_request=lora_request, tools=tools, temperature=0, top_p=1, max_new_tokens=768)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 11:30:11 scheduler.py:1754] Sequence group 22 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/139 [00:06<14:47,  6.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2/139 [00:11<12:55,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/139 [00:16<12:08,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4/139 [00:21<11:42,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 5/139 [00:26<11:15,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 6/139 [00:31<11:38,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:30:42 scheduler.py:1754] Sequence group 308 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 7/139 [00:37<11:33,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 8/139 [00:42<11:18,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 9/139 [00:47<11:08,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 10/139 [00:53<11:27,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 11/139 [00:58<11:26,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 12/139 [01:05<12:25,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:31:16 scheduler.py:1754] Sequence group 597 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 15/139 [01:15<09:04,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 16/139 [01:20<09:24,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 17/139 [01:25<09:55,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 18/139 [01:31<10:29,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:31:42 scheduler.py:1754] Sequence group 884 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 19/139 [01:37<10:56,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 20/139 [01:43<10:58,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 21/139 [01:48<10:31,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 22/139 [01:54<10:42,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 23/139 [01:59<10:18,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 25/139 [02:06<08:37,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:32:17 scheduler.py:1754] Sequence group 1242 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 26/139 [02:11<08:39,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 28/139 [02:18<07:44,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 29/139 [02:23<08:08,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 30/139 [02:29<08:46,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 31/139 [02:34<08:52,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 32/139 [02:39<09:12,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:32:51 scheduler.py:1754] Sequence group 1567 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 33/139 [02:45<09:25,  5.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 34/139 [02:50<09:13,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 35/139 [02:55<09:06,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 36/139 [03:02<09:52,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 37/139 [03:08<09:46,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 38/139 [03:14<09:36,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:33:26 scheduler.py:1754] Sequence group 1870 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 39/139 [03:19<09:04,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 40/139 [03:23<08:35,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 41/139 [03:30<09:18,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 43/139 [03:37<07:25,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 44/139 [03:41<07:20,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 45/139 [03:46<07:26,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:33:58 scheduler.py:1754] Sequence group 2194 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 46/139 [03:52<07:32,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 47/139 [03:57<07:39,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 48/139 [04:02<07:36,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 49/139 [04:07<07:39,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 50/139 [04:13<08:03,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 51/139 [04:16<06:32,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 11:34:26 scheduler.py:1754] Sequence group 2467 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 52/139 [04:21<06:46,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 54/139 [04:28<05:57,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 55/139 [04:33<06:12,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 56/139 [04:38<06:18,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 57/139 [04:43<06:25,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:34:54 scheduler.py:1754] Sequence group 2761 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 58/139 [04:48<06:40,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 59/139 [04:53<06:40,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 60/139 [04:59<06:55,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 61/139 [05:04<06:42,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 62/139 [05:09<06:37,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 63/139 [05:14<06:29,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:35:26 scheduler.py:1754] Sequence group 3063 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 64/139 [05:19<06:18,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 65/139 [05:24<06:16,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 68/139 [05:34<04:47,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 69/139 [05:39<05:05,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 70/139 [05:44<05:21,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:35:55 scheduler.py:1754] Sequence group 3387 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 71/139 [05:51<06:05,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 72/139 [05:57<05:58,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 74/139 [06:05<05:18,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 75/139 [06:11<05:35,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:36:22 scheduler.py:1754] Sequence group 3627 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=601\n",
      "WARNING 05-26 11:36:22 scheduler.py:1091] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 76/139 [06:16<05:19,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 77/139 [06:21<05:13,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 78/139 [06:26<05:06,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 79/139 [06:31<05:07,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 80/139 [06:36<05:01,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 81/139 [06:41<04:55,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 82/139 [06:46<04:46,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 83/139 [06:51<04:40,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:37:02 scheduler.py:1754] Sequence group 4020 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 85/139 [06:58<04:03,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 86/139 [07:04<04:11,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 87/139 [07:09<04:17,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 88/139 [07:14<04:11,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 89/139 [07:19<04:08,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 90/139 [07:24<04:03,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:37:35 scheduler.py:1754] Sequence group 4340 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 92/139 [07:32<03:37,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 93/139 [07:37<03:41,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 94/139 [07:42<03:39,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 95/139 [07:48<03:41,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:37:59 scheduler.py:1754] Sequence group 4601 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 96/139 [07:53<03:39,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 97/139 [07:58<03:33,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 98/139 [08:04<03:32,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 99/139 [08:09<03:29,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 100/139 [08:14<03:18,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 101/139 [08:19<03:18,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 102/139 [08:24<03:09,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:38:35 scheduler.py:1754] Sequence group 4917 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 103/139 [08:29<03:02,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 105/139 [08:36<02:31,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 106/139 [08:41<02:33,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 107/139 [08:46<02:34,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 108/139 [08:52<02:36,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 109/139 [08:57<02:35,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:39:09 scheduler.py:1754] Sequence group 5274 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 110/139 [09:02<02:27,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 111/139 [09:07<02:23,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 112/139 [09:12<02:16,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 113/139 [09:18<02:15,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 114/139 [09:23<02:09,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 115/139 [09:28<02:02,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 116/139 [09:33<01:55,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:39:44 scheduler.py:1754] Sequence group 5605 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 117/139 [09:38<01:48,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 118/139 [09:43<01:49,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 119/139 [09:49<01:44,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 120/139 [09:54<01:39,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 121/139 [10:00<01:36,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 123/139 [10:06<01:11,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:40:17 scheduler.py:1754] Sequence group 5924 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 124/139 [10:11<01:08,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 125/139 [10:17<01:07,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 126/139 [10:22<01:05,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 127/139 [10:27<01:00,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 128/139 [10:32<00:55,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:40:43 scheduler.py:1754] Sequence group 6168 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 130/139 [10:40<00:40,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 131/139 [10:44<00:36,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 132/139 [10:49<00:32,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 134/139 [10:57<00:21,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-26 11:41:09 scheduler.py:1754] Sequence group 6479 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 135/139 [11:02<00:18,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 136/139 [11:06<00:13,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 138/139 [11:14<00:04,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [11:14<00:00,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score: 8.1556\n",
      "Average tools used: 0.0000\n",
      "Average calls per sample: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from pretrained peft model\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "\n",
    "vllm_lora_adapter = 'models/best_policy_model10'\n",
    "base_model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "inference_engine = LLM(\n",
    "    model=base_model_name,\n",
    "    enable_lora=True,\n",
    "    max_lora_rank=64,\n",
    "    max_loras=1,\n",
    "    gpu_memory_utilization=0.2,\n",
    "    # enable_prefix_caching=True,\n",
    "    swap_space=6,\n",
    "    scheduling_policy=\"fcfs\",\n",
    "    dtype=torch.bfloat16,\n",
    "    max_model_len=2048,\n",
    "    # enable_sleep_mode=True,\n",
    "    )\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=48, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "avg_bleu, tools_used_avg, calls_per_sample_avg = evaluate_model(inference_engine, tokenizer, dataloader, actions_num=4, lora_request=LoRARequest('adapter', 1, vllm_lora_adapter), tools=TOOLS)\n",
    "print(f\"Average BLEU score: {avg_bleu:.4f}\")\n",
    "print(f\"Average tools used: {tools_used_avg:.4f}\")\n",
    "print(f\"Average calls per sample: {calls_per_sample_avg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe77d38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
