{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffda8ffc",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e904aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, spa_path, wayuu_path):\n",
    "        with open(spa_path, 'r', encoding='utf-8') as f:\n",
    "            self.spa_lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "        with open(wayuu_path, 'r', encoding='utf-8') as f:\n",
    "            self.wayuu_lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spa_lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        spa = self.spa_lines[idx]\n",
    "        wayuu = self.wayuu_lines[idx]\n",
    "        \n",
    "        return spa, wayuu\n",
    "    \n",
    "spanish_val_file = 'datasets/dev.es.txt'\n",
    "wayuu_val_file = 'datasets/dev.guc.txt'\n",
    "\n",
    "# Load the dataset\n",
    "dataset = TextDataset(spanish_val_file, wayuu_val_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d81292",
   "metadata": {},
   "source": [
    "### Eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bf39269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manuel/projects/rl_translator/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-21 15:19:29,715\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sacrebleu\n",
    "from tqdm import tqdm\n",
    "from vllm import SamplingParams\n",
    "\n",
    "def get_rewards_translation(generations, correct_translations):\n",
    "\n",
    "    bleu = sacrebleu.BLEU(effective_order = True)\n",
    "    def get_bleu_score(sample, correct_translation):\n",
    "        # Compute bleu score for each sample. \n",
    "        # Bleu score normalized to [0, 1]\n",
    "        return bleu.sentence_score(sample, \n",
    "                                   [correct_translation]\n",
    "                                   ).score\n",
    "\n",
    "    answer_bleu_scores = [\n",
    "        get_bleu_score(sample, translation)\n",
    "        for sample, translation in zip(generations, correct_translations)\n",
    "    ]\n",
    "    \n",
    "    return answer_bleu_scores\n",
    "\n",
    "translate_prompt_template_tool=\"\"\"Translate the following Spanish text into Wayuunaiki.\n",
    "Begin by identifying any words or phrases you're unsure how to translate. Then, you may look up those words using the dictionary tool by wrapping the Spanish word in <spa_to_wayuu> and </spa_to_wayuu>,\n",
    "and doind that for every unknown word. The dictionary will return matches enclosed in <matches> and </matches>. You can use the dictionary as many times as necessary.\n",
    "Once you have all the information you need, provide the final translation enclosed in <answer> and </answer>. For example: <answer> xxx </answer>.\n",
    "\n",
    "Spanish text: {}\"\"\"\n",
    "def generate_batch_completion(model, tokenizer, prompts: list, actions_num=1, **kwargs):\n",
    "    batch = [[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": translate_prompt_template_tool.format(prompt)}\n",
    "    ] for prompt in prompts]\n",
    "    texts = tokenizer.apply_chat_template(\n",
    "        batch,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    default_sampling_args = {\n",
    "        'max_new_tokens': 512,\n",
    "        'temperature': 0.8,\n",
    "        'top_p': 0.95,\n",
    "    }\n",
    "    default_sampling_args.update(kwargs)\n",
    "\n",
    "    model_inputs = tokenizer(texts)\n",
    "\n",
    "    inputs = model_inputs.input_ids\n",
    "    dones = [False] * len(prompts)\n",
    "    prompt_length = [len(input_ids) for input_ids in inputs]\n",
    "    mask = [[1] * len(input_ids) for input_ids in inputs]\n",
    "    responses = [\"\"] * len(prompts)\n",
    "    tools_enabled = kwargs.get('tools', [])\n",
    "    stop_tokens = [tool['end_token'] for tool in tools_enabled]\n",
    "    tool_used = [False] * len(prompts)\n",
    "    how_many_tool_calls = [0] * len(prompts)\n",
    "    for action_step in range(actions_num + 1 if len(tools_enabled) > 0 else 1):\n",
    "        sampling_params = SamplingParams(temperature=default_sampling_args[\"temperature\"], top_p=default_sampling_args['top_p'], top_k=-1, max_tokens=default_sampling_args['max_new_tokens'],\n",
    "            stop=stop_tokens)\n",
    "        outputs = model.generate(prompt_token_ids=inputs, sampling_params=sampling_params, lora_request=kwargs['lora_request'], use_tqdm=False)\n",
    "\n",
    "        for j, output in enumerate(outputs):\n",
    "            if dones[j]:\n",
    "                continue\n",
    "            \n",
    "            for tool in tools_enabled:\n",
    "                if output.outputs[0].stop_reason == tool['end_token'] and tool['start_token'] in output.outputs[0].text:\n",
    "                    api_args = output.outputs[0].text.split(tool['start_token'])[1].strip()\n",
    "                    api_result = tool['api'](api_args)\n",
    "                    # responses[j] += f\"{tool['start_token']} \" + api_args + f\" {tool['end_token']}\" + api_result\n",
    "                    responses[j] += output.outputs[0].text + f\"{tool['end_token']}\" + api_result\n",
    "                    api_result_tokens = tokenizer.encode(api_result, return_tensors=None)\n",
    "                    inputs[j] += list(output.outputs[0].token_ids) + api_result_tokens\n",
    "\n",
    "                    tool_used[j] = True\n",
    "                    how_many_tool_calls[j] += 1\n",
    "                    break # Only one tool can be used at a time\n",
    "            if output.outputs[0].finish_reason == \"stop\" and output.outputs[0].stop_reason is None:\n",
    "                responses[j] += output.outputs[0].text\n",
    "                inputs[j] += list(output.outputs[0].token_ids)\n",
    "                dones[j] = True\n",
    "            elif output.outputs[0].stop_reason not in stop_tokens:\n",
    "                print(f\"Unexpected finish reason: {output.outputs[0].finish_reason} {output.outputs[0].stop_reason}\")\n",
    "                responses[j] += tokenizer.eos_token\n",
    "                inputs[j] += [tokenizer.eos_token_id]\n",
    "                dones[j] = True\n",
    "\n",
    "    return responses, tool_used, how_many_tool_calls\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_answer(response, transform_fn = lambda x: x, nan_val = None)->str|None:\n",
    "    ans = re.match('.*?<answer>(.*?)</answer>\\s*$', response, re.DOTALL|re.MULTILINE)\n",
    "    if ans:\n",
    "        try:\n",
    "            return transform_fn(ans[1].strip())\n",
    "        except:\n",
    "            return nan_val\n",
    "    return nan_val\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataloader, actions_num=1, lora_request=None, tools=None):\n",
    "    sum_bleu = 0\n",
    "    num_samples = 0\n",
    "    tools_used_in_total = 0\n",
    "    calls_per_sample = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            inputs, targets = batch\n",
    "\n",
    "            # Generate translations\n",
    "            outputs, tools_used, how_many_tool_calls = generate_batch_completion(model, tokenizer, inputs, actions_num=actions_num, lora_request=lora_request, tools=tools, temperature=0, top_p=1, max_new_tokens=768)\n",
    "\n",
    "            tools_used_in_total += sum(tools_used)\n",
    "            calls_per_sample += sum(how_many_tool_calls)\n",
    "\n",
    "            generated_translations = [\n",
    "                extract_answer(output, transform_fn=lambda x: x.strip(), nan_val='')\n",
    "                for output in outputs\n",
    "            ]\n",
    "            # Calculate BLEU scores\n",
    "            bleu_scores = get_rewards_translation(generated_translations, targets)\n",
    "            \n",
    "            sum_bleu += sum(bleu_scores)\n",
    "            num_samples += len(bleu_scores)\n",
    "    avg_bleu = sum_bleu / num_samples if num_samples > 0 else 0\n",
    "    tools_used_avg = tools_used_in_total / num_samples\n",
    "    calls_per_sample_avg = calls_per_sample / tools_used_in_total\n",
    "    return avg_bleu, tools_used_avg, calls_per_sample_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0681dd90",
   "metadata": {},
   "source": [
    "### tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a3e47b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spa_to_wayu_dictionary(spanish_word, max_matches=5):\n",
    "    dictionary_path = 'assets/spanish_to_wayuunaiki_short.csv'\n",
    "\n",
    "    with open(dictionary_path, 'r', encoding='utf-8') as f:\n",
    "        all_matches = []\n",
    "        line = f.readline()\n",
    "        while line != '' and len(all_matches) < max_matches:\n",
    "            data = line.strip().split(',')\n",
    "            if re.search(rf'\\b{re.escape(spanish_word)}\\b', data[0], re.IGNORECASE):\n",
    "                all_matches.append(data)\n",
    "            line = f.readline()\n",
    "\n",
    "    if len(all_matches) > 0:\n",
    "        result = \" <matches> \" + '\\n'.join(f'{spa}: {wayuu}' for spa, wayuu in all_matches) + \" </matches>\"\n",
    "        # print(f'CORRECT USE OF SPA_TO_WAYU TOOL. Word: {spanish_word}, Result: {result}')\n",
    "    else:\n",
    "        result = \" <matches> No matches found </matches>\"\n",
    "        # print(f'NO_MATCHES SPA_TO_WAYU TOOL. Word: {spanish_word}')\n",
    "\n",
    "    return result\n",
    "\n",
    "TOOLS = [\n",
    "    {\n",
    "        'name': 'spa_to_wayu',\n",
    "        'description': 'A tool that translates a word from Spanish to Wayuunaiki.',\n",
    "        'api': spa_to_wayu_dictionary,\n",
    "        'start_token': '<spa_to_wayuu>',\n",
    "        'end_token': '</spa_to_wayuu>',\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e573b92",
   "metadata": {},
   "source": [
    "### Model with SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7707bb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-21 14:53:23,603] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-21 14:53:24 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 05-21 14:53:28 config.py:549] This model supports multiple tasks: {'generate', 'reward', 'classify', 'embed', 'score'}. Defaulting to 'generate'.\n",
      "INFO 05-21 14:53:28 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='Qwen/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-0.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 05-21 14:53:29 interface.py:304] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 05-21 14:53:29 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 05-21 14:53:29 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-0.5B-Instruct...\n",
      "INFO 05-21 14:53:29 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 05-21 14:53:30 weight_utils.py:304] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.57it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-21 14:53:30 model_runner.py:1115] Loading model weights took 0.9277 GB\n",
      "INFO 05-21 14:53:30 punica_selector.py:18] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-21 14:53:31 worker.py:267] Memory profiling takes 0.88 seconds\n",
      "INFO 05-21 14:53:31 worker.py:267] the current vLLM instance can use total_gpu_memory (11.99GiB) x gpu_memory_utilization (0.20) = 2.40GiB\n",
      "INFO 05-21 14:53:31 worker.py:267] model weights take 0.93GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 0.04GiB.\n",
      "INFO 05-21 14:53:31 executor_base.py:111] # cuda blocks: 233, # CPU blocks: 32768\n",
      "INFO 05-21 14:53:31 executor_base.py:116] Maximum concurrency for 768 tokens per request: 4.85x\n",
      "INFO 05-21 14:53:32 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:11<00:00,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-21 14:53:43 model_runner.py:1562] Graph capturing finished in 11 secs, took 0.31 GiB\n",
      "INFO 05-21 14:53:43 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 12.69 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/104 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 14:53:43 tokenizer.py:264] No tokenizer found in models/sft_base_qwen2, using base model tokenizer instead. (Exception: Can't load tokenizer for 'models/sft_base_qwen2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'models/sft_base_qwen2' is the correct path to a directory containing all relevant files for a Qwen2TokenizerFast tokenizer.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_123978/4290022123.py:113: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.\n",
      "  outputs, tools_used, how_many_tool_calls = generate_batch_completion(model, tokenizer, inputs, actions_num=actions_num, lora_request=lora_request, tools=tools, temperature=0, top_p=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 14:53:44 scheduler.py:1754] Sequence group 41 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/104 [00:07<13:43,  8.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/104 [00:18<16:04,  9.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/104 [00:31<18:27, 10.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/104 [00:45<20:24, 12.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 5/104 [00:58<20:37, 12.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 14:54:45 scheduler.py:1754] Sequence group 1743 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/104 [01:07<18:44, 11.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/104 [01:27<16:29, 10.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 9/104 [01:40<17:57, 11.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 10/104 [01:50<16:53, 10.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 14:55:39 scheduler.py:1754] Sequence group 3385 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/104 [02:03<17:35, 11.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/104 [02:12<16:18, 10.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 13/104 [02:23<16:22, 10.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 14/104 [02:34<16:34, 11.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 15/104 [02:49<18:00, 12.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 16/104 [03:02<18:02, 12.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 14:56:46 scheduler.py:1754] Sequence group 5140 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 17/104 [03:13<17:30, 12.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 18/104 [03:26<17:40, 12.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 19/104 [03:36<16:13, 11.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 21/104 [03:53<13:49,  9.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 14:57:40 scheduler.py:1754] Sequence group 6843 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 22/104 [04:04<13:45, 10.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 23/104 [04:16<14:22, 10.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 24/104 [04:30<15:31, 11.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 25/104 [04:42<15:39, 11.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 26/104 [04:53<14:55, 11.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 14:58:40 scheduler.py:1754] Sequence group 8485 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=251\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 27/104 [05:02<14:01, 10.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 28/104 [05:16<14:45, 11.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 29/104 [05:28<14:49, 11.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 30/104 [05:40<14:37, 11.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 31/104 [05:53<14:52, 12.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 14:59:37 scheduler.py:1754] Sequence group 9940 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=301\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 32/104 [06:05<14:42, 12.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 33/104 [06:19<15:03, 12.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 35/104 [06:40<13:03, 11.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 36/104 [06:50<12:33, 11.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:00:35 scheduler.py:1754] Sequence group 11579 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=351\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 37/104 [07:02<12:37, 11.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 38/104 [07:12<11:46, 10.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 39/104 [07:22<11:23, 10.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 40/104 [07:36<12:32, 11.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 41/104 [07:48<12:29, 11.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:01:32 scheduler.py:1754] Sequence group 13140 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=401\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 44/104 [08:17<10:03, 10.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 45/104 [08:30<10:36, 10.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:02:19 scheduler.py:1754] Sequence group 14588 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=451\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 47/104 [08:50<09:41, 10.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 48/104 [09:02<09:58, 10.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 49/104 [09:15<10:37, 11.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 50/104 [09:28<10:38, 11.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 51/104 [09:41<10:44, 12.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:03:25 scheduler.py:1754] Sequence group 16381 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=501\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 52/104 [09:54<10:53, 12.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 53/104 [10:08<11:01, 12.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 55/104 [10:29<09:17, 11.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:04:16 scheduler.py:1754] Sequence group 17743 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=551\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 56/104 [10:42<09:36, 12.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:04:26 scheduler.py:1091] Input prompt (2049 tokens) is too long and exceeds limit of 768\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-21 15:04:28 scheduler.py:1091] Input prompt (2050 tokens) is too long and exceeds limit of 768\n",
      "WARNING 05-21 15:04:31 scheduler.py:1091] Input prompt (2050 tokens) is too long and exceeds limit of 768\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-21 15:04:34 scheduler.py:1091] Input prompt (2050 tokens) is too long and exceeds limit of 768\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-21 15:04:37 scheduler.py:1091] Input prompt (2050 tokens) is too long and exceeds limit of 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 57/104 [10:54<09:26, 12.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 59/104 [11:14<08:07, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 60/104 [11:25<07:58, 10.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 61/104 [11:35<07:34, 10.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:05:19 scheduler.py:1754] Sequence group 19559 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=601\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 62/104 [11:47<07:42, 11.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 63/104 [11:58<07:27, 10.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:05:42 scheduler.py:1091] Input prompt (1514 tokens) is too long and exceeds limit of 768\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-21 15:05:43 scheduler.py:1091] Input prompt (1515 tokens) is too long and exceeds limit of 768\n",
      "WARNING 05-21 15:05:46 scheduler.py:1091] Input prompt (1515 tokens) is too long and exceeds limit of 768\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-21 15:05:50 scheduler.py:1091] Input prompt (1515 tokens) is too long and exceeds limit of 768\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-21 15:05:52 scheduler.py:1091] Input prompt (1515 tokens) is too long and exceeds limit of 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 64/104 [12:09<07:21, 11.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 65/104 [12:23<07:43, 11.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:06:11 scheduler.py:1754] Sequence group 20970 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=651\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 66/104 [12:36<07:43, 12.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 67/104 [12:48<07:26, 12.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 68/104 [12:59<07:05, 11.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 69/104 [13:10<06:47, 11.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 70/104 [13:23<06:45, 11.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:07:09 scheduler.py:1754] Sequence group 22497 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=701\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 71/104 [13:35<06:40, 12.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 72/104 [13:45<06:03, 11.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 73/104 [13:57<05:56, 11.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 74/104 [14:09<05:54, 11.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 75/104 [14:22<05:46, 11.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 76/104 [14:34<05:37, 12.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:08:18 scheduler.py:1754] Sequence group 24340 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=751\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 77/104 [14:46<05:23, 11.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 78/104 [14:55<04:46, 11.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 79/104 [15:07<04:47, 11.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 80/104 [15:17<04:24, 11.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:09:02 scheduler.py:1754] Sequence group 25661 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=801\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 81/104 [15:29<04:20, 11.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 82/104 [15:40<04:06, 11.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 83/104 [15:52<03:57, 11.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 84/104 [16:03<03:45, 11.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 86/104 [16:22<03:05, 10.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:10:06 scheduler.py:1754] Sequence group 27558 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=851\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 87/104 [16:33<02:56, 10.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 88/104 [16:47<03:02, 11.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 89/104 [16:59<02:53, 11.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 90/104 [17:11<02:43, 11.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 91/104 [17:23<02:33, 11.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:11:07 scheduler.py:1754] Sequence group 29159 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=901\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 93/104 [17:43<01:58, 10.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 94/104 [17:57<01:57, 11.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:11:42 scheduler.py:1091] Input prompt (834 tokens) is too long and exceeds limit of 768\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-21 15:11:44 scheduler.py:1091] Input prompt (835 tokens) is too long and exceeds limit of 768\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-21 15:11:49 scheduler.py:1091] Input prompt (835 tokens) is too long and exceeds limit of 768\n",
      "WARNING 05-21 15:11:52 scheduler.py:1091] Input prompt (835 tokens) is too long and exceeds limit of 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 96/104 [18:19<01:27, 10.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:12:03 scheduler.py:1754] Sequence group 30760 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=951\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 97/104 [18:31<01:20, 11.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 98/104 [18:43<01:09, 11.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 99/104 [18:55<00:58, 11.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 100/104 [19:05<00:45, 11.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 101/104 [19:15<00:32, 10.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:13:00 scheduler.py:1754] Sequence group 32411 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1001\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 103/104 [19:35<00:10, 10.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [19:43<00:00, 11.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score: 13.2998\n",
      "Average tools used: 0.9025\n",
      "Average calls per sample: 2.1136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from pretrained peft model\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "\n",
    "# def load_model(model_name_or_path, peft_model_id):\n",
    "#     # Load the base model\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\", torch_dtype='auto')\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "#     # Load the PEFT model\n",
    "#     peft_model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "    \n",
    "#     return peft_model, tokenizer\n",
    "\n",
    "\n",
    "vllm_lora_adapter = 'models/sft_base_qwen2'\n",
    "base_model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "inference_engine = LLM(\n",
    "    model=base_model_name,\n",
    "    enable_lora=True,\n",
    "    max_lora_rank=64,\n",
    "    max_loras=1,\n",
    "    gpu_memory_utilization=0.2,\n",
    "    # enable_prefix_caching=True,\n",
    "    swap_space=6,\n",
    "    scheduling_policy=\"fcfs\",\n",
    "    dtype=torch.bfloat16,\n",
    "    max_model_len=2060,\n",
    "    # enable_sleep_mode=True,\n",
    "    )\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "avg_bleu, tools_used_avg, calls_per_sample_avg = evaluate_model(inference_engine, tokenizer, dataloader, actions_num=4, lora_request=LoRARequest('adapter', 1, vllm_lora_adapter), tools=TOOLS)\n",
    "print(f\"Average BLEU score: {avg_bleu:.4f}\")\n",
    "print(f\"Average tools used: {tools_used_avg:.4f}\")\n",
    "print(f\"Average calls per sample: {calls_per_sample_avg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7da6b6",
   "metadata": {},
   "source": [
    "## Model with SFT + RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57afea76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-21 15:20:23,860] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-21 15:20:24 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 05-21 15:20:29 config.py:549] This model supports multiple tasks: {'score', 'embed', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 05-21 15:20:29 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='Qwen/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-0.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 05-21 15:20:29 interface.py:304] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 05-21 15:20:29 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 05-21 15:20:30 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-0.5B-Instruct...\n",
      "INFO 05-21 15:20:30 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 05-21 15:20:30 weight_utils.py:304] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.90it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-21 15:20:31 model_runner.py:1115] Loading model weights took 0.9277 GB\n",
      "INFO 05-21 15:20:31 punica_selector.py:18] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-21 15:20:32 worker.py:267] Memory profiling takes 0.93 seconds\n",
      "INFO 05-21 15:20:32 worker.py:267] the current vLLM instance can use total_gpu_memory (11.99GiB) x gpu_memory_utilization (0.20) = 2.40GiB\n",
      "INFO 05-21 15:20:32 worker.py:267] model weights take 0.93GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 0.04GiB.\n",
      "INFO 05-21 15:20:32 executor_base.py:111] # cuda blocks: 233, # CPU blocks: 32768\n",
      "INFO 05-21 15:20:32 executor_base.py:116] Maximum concurrency for 2048 tokens per request: 1.82x\n",
      "INFO 05-21 15:20:32 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:11<00:00,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-21 15:20:44 model_runner.py:1562] Graph capturing finished in 12 secs, took 0.31 GiB\n",
      "INFO 05-21 15:20:44 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 13.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/139 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:20:44 tokenizer.py:264] No tokenizer found in models/grpo_policy_model5, using base model tokenizer instead. (Exception: Unrecognized model in models/grpo_policy_model5. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_135942/3822158594.py:113: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.\n",
      "  outputs, tools_used, how_many_tool_calls = generate_batch_completion(model, tokenizer, inputs, actions_num=actions_num, lora_request=lora_request, tools=tools, temperature=0, top_p=1, max_new_tokens=768)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:20:45 scheduler.py:1754] Sequence group 41 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/139 [00:09<22:23,  9.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2/139 [00:19<21:56,  9.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/139 [00:28<21:36,  9.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4/139 [00:44<26:45, 11.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 5/139 [00:54<25:02, 11.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 6/139 [01:09<28:04, 12.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 7/139 [01:26<30:36, 13.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:22:13 scheduler.py:1754] Sequence group 1789 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 10/139 [01:43<17:43,  8.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 11/139 [01:52<18:13,  8.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 13/139 [02:11<18:13,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 14/139 [02:24<20:53, 10.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:23:12 scheduler.py:1754] Sequence group 3490 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 16/139 [02:45<20:18,  9.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 17/139 [02:54<19:33,  9.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 18/139 [03:03<19:09,  9.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 19/139 [03:13<19:22,  9.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 20/139 [03:29<22:48, 11.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 21/139 [03:44<24:53, 12.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:24:30 scheduler.py:1754] Sequence group 5081 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 22/139 [04:00<26:26, 13.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 23/139 [04:10<24:08, 12.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 24/139 [04:26<25:47, 13.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 26/139 [04:40<18:45,  9.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 27/139 [04:49<18:05,  9.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 28/139 [04:58<17:31,  9.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:25:44 scheduler.py:1754] Sequence group 6798 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 30/139 [05:16<16:23,  9.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 31/139 [05:29<18:19, 10.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 32/139 [05:45<21:32, 12.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 33/139 [05:55<19:49, 11.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 34/139 [06:11<22:15, 12.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 35/139 [06:26<23:32, 13.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 36/139 [06:36<21:17, 12.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:27:22 scheduler.py:1754] Sequence group 8730 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=251\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 37/139 [06:52<22:59, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 38/139 [07:02<20:59, 12.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 39/139 [07:12<19:36, 11.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 40/139 [07:25<19:55, 12.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 41/139 [07:40<20:59, 12.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 43/139 [08:00<17:47, 11.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:28:49 scheduler.py:1754] Sequence group 10459 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=301\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 44/139 [08:16<19:40, 12.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 45/139 [08:32<21:20, 13.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 47/139 [08:55<18:21, 11.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 48/139 [09:10<19:32, 12.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 49/139 [09:19<17:41, 11.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 50/139 [09:32<17:58, 12.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:30:18 scheduler.py:1754] Sequence group 12091 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=351\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 51/139 [09:45<18:00, 12.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 52/139 [10:01<19:32, 13.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 53/139 [10:18<21:02, 14.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 54/139 [10:33<20:52, 14.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 55/139 [10:42<18:17, 13.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 56/139 [10:59<19:21, 13.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:31:44 scheduler.py:1754] Sequence group 13503 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 59/139 [11:16<11:29,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 60/139 [11:31<13:57, 10.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 61/139 [11:40<13:17, 10.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 63/139 [12:01<12:32,  9.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:32:47 scheduler.py:1754] Sequence group 15198 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=451\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 64/139 [12:11<12:12,  9.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 65/139 [12:29<15:13, 12.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 66/139 [12:39<14:05, 11.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 67/139 [12:49<13:18, 11.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 70/139 [13:10<09:20,  8.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:33:56 scheduler.py:1754] Sequence group 16874 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=501\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 71/139 [13:27<12:16, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 73/139 [13:43<10:08,  9.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 74/139 [13:59<12:08, 11.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 75/139 [14:14<13:12, 12.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:34:59 scheduler.py:1091] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-21 15:35:00 scheduler.py:1091] Input prompt (2050 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-21 15:35:02 scheduler.py:1091] Input prompt (2050 tokens) is too long and exceeds limit of 2048\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "WARNING 05-21 15:35:06 scheduler.py:1091] Input prompt (2050 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-21 15:35:07 scheduler.py:1091] Input prompt (2050 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 76/139 [14:23<12:01, 11.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 77/139 [14:33<11:16, 10.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:35:20 scheduler.py:1754] Sequence group 18590 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=551\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 78/139 [14:48<12:20, 12.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 79/139 [15:03<13:11, 13.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 80/139 [15:13<11:57, 12.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 81/139 [15:22<10:49, 11.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 82/139 [15:37<11:50, 12.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 84/139 [15:52<08:57,  9.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 85/139 [16:05<09:29, 10.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:36:52 scheduler.py:1754] Sequence group 20493 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 86/139 [16:11<08:08,  9.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 87/139 [16:27<09:42, 11.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 88/139 [16:42<10:35, 12.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 89/139 [16:52<09:36, 11.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 91/139 [17:17<09:10, 11.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:38:06 scheduler.py:1754] Sequence group 21972 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=651\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 93/139 [17:38<08:08, 10.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 94/139 [17:54<09:03, 12.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 95/139 [18:09<09:36, 13.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 96/139 [18:19<08:40, 12.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 97/139 [18:35<09:13, 13.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 99/139 [18:56<07:35, 11.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:39:41 scheduler.py:1754] Sequence group 23801 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=701\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 100/139 [19:11<08:15, 12.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 102/139 [19:33<06:57, 11.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 104/139 [19:49<05:34,  9.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 105/139 [20:01<05:50, 10.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:40:49 scheduler.py:1754] Sequence group 25324 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=751\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 107/139 [20:22<05:18,  9.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 109/139 [20:43<04:50,  9.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 110/139 [20:55<05:03, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 111/139 [21:11<05:41, 12.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 112/139 [21:27<05:54, 13.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 113/139 [21:40<05:39, 13.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:42:25 scheduler.py:1754] Sequence group 27140 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=801\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 115/139 [22:01<04:33, 11.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 116/139 [22:10<04:05, 10.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 117/139 [22:25<04:26, 12.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 118/139 [22:42<04:43, 13.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 119/139 [22:51<04:03, 12.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 120/139 [23:03<03:50, 12.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:43:48 scheduler.py:1754] Sequence group 28820 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=851\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 121/139 [23:16<03:41, 12.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 122/139 [23:31<03:42, 13.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 124/139 [23:52<02:49, 11.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 125/139 [24:08<02:57, 12.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 126/139 [24:28<03:16, 15.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 127/139 [24:38<02:41, 13.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:45:23 scheduler.py:1754] Sequence group 30501 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 128/139 [24:44<02:02, 11.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 129/139 [24:56<01:56, 11.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 130/139 [25:09<01:46, 11.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 132/139 [25:26<01:10, 10.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 133/139 [25:36<00:59,  9.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 134/139 [25:51<00:57, 11.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-21 15:46:36 scheduler.py:1754] Sequence group 32200 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=951\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 135/139 [26:01<00:44, 11.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n",
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 137/139 [26:17<00:18,  9.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected finish reason: length None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [26:29<00:00, 11.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score: 13.3104\n",
      "Average tools used: 0.9017\n",
      "Average calls per sample: 2.1093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from pretrained peft model\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "\n",
    "vllm_lora_adapter = 'models/grpo_policy_model5'\n",
    "base_model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "inference_engine = LLM(\n",
    "    model=base_model_name,\n",
    "    enable_lora=True,\n",
    "    max_lora_rank=64,\n",
    "    max_loras=1,\n",
    "    gpu_memory_utilization=0.2,\n",
    "    # enable_prefix_caching=True,\n",
    "    swap_space=6,\n",
    "    scheduling_policy=\"fcfs\",\n",
    "    dtype=torch.bfloat16,\n",
    "    max_model_len=2048,\n",
    "    # enable_sleep_mode=True,\n",
    "    )\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=48, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "avg_bleu, tools_used_avg, calls_per_sample_avg = evaluate_model(inference_engine, tokenizer, dataloader, actions_num=4, lora_request=LoRARequest('adapter', 1, vllm_lora_adapter), tools=TOOLS)\n",
    "print(f\"Average BLEU score: {avg_bleu:.4f}\")\n",
    "print(f\"Average tools used: {tools_used_avg:.4f}\")\n",
    "print(f\"Average calls per sample: {calls_per_sample_avg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db88073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
